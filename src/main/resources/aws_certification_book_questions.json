{
  "Chapter 6": [
    {
      "question": "Which of the following AWS services enables you to automate your build, test, deploy, and \r\nrelease process every time there is a code change?\r\nA. AWS CodeCommit\r\nB. AWS CodeDeploy\r\nC. AWS CodeBuild\r\nD. AWS CodePipeline",
      "answer": "D. Option D is correct because AWS CodePipeline is a continuous delivery service for fast \r\nand reliable application updates. It allows the developer to model and visualize the software \r\nrelease process. CodePipeline automates your build, test, and release process when there is a \r\ncode change.\r\nOption A is incorrect because AWS CodeCommit is a secure, highly scalable, managed \r\nsource control service that hosts private Git repositories.\r\nOption B is incorrect because AWS CodeDeploy automates code deployments to any \r\ninstance and handles the complexity of updating your applications.\r\nOption C is incorrect because AWS CodeBuild compiles source code, runs tests, and pro-\r\nduces ready-to-deploy software packages."
    },
    {
      "question": "Which of the following resources can AWS Elastic Beanstalk use to create a web server \r\nenvironment? (Select FOUR.)\r\nA. Amazon Cognito User Pool\r\nB. AWS Serverless Application Model (AWS SAM) Local\r\nC. Auto Scaling group\r\nD. Amazon Elastic Compute Cloud (Amazon EC2)\r\nE. AWS Lambda",
      "answer": "A, B, C, D. A, B, C, and D are correct because you can use them all to create a web server \r\nenvironment with AWS Elastic Beanstalk.\r\nOption E is incorrect because AWS Lambda is an event-driven, serverless computing plat-\r\nform that runs code in response to events. Lambda automatically manages the computing \r\nresources required by that code."
    },
    {
      "question": "Which of the following languages is not supported by AWS Elastic Beanstalk?\r\nA. Java\r\nB. Node.js\r\nC. Objective C\r\nD. Go",
      "answer": "C. Elastic Beanstalk supports Java, Node.js, and Go, so options A, B, and D are incorrect. \r\nIt does not support Objective C, so option C is the correct answer."
    },
    {
      "question": "What does the AWS Elastic Beanstalk service do?\r\nA. Deploys applications and architecture\r\nB. Stores static content\r\nC. Directs user traffic to Amazon Elastic Compute Cloud (Amazon EC2) instances\r\nD. Works with dynamic cloud changes as an IP address",
      "answer": "A. Elastic Beanstalk deploys application code and the architecture to support an environ-\r\nment for the application to run."
    },
    {
      "question": "Which operating systems does AWS Elastic Beanstalk support? (Select TWO.)\r\nA. Amazon Linux\r\nB. Ubuntu\r\nC. Windows Server\r\nD. Fedora\r\nE. Jetty\r\n",
      "answer": "A, C. Elastic Beanstalk supports Linux and Windows. No support is available for an \r\nUbuntu-only operating system, Fedora, or Jetty."
    },
    {
      "question": "Which of the following components can AWS Elastic Beanstalk deploy? (Select TWO.)\r\nA. Amazon Elastic Compute Cloud (Amazon EC2) instances with write capabilities to  \r\nan Amazon DynamoDB table\r\nB. A worker application using Amazon Simple Queue Service (Amazon SQS)\r\nC. An Amazon Elastic Container Service (Amazon ECS) cluster supporting  \r\nmultiple containers\r\nD. A mixed fleet of Spot and Reserved Instances with four applications running in each \r\nenvironment\r\nE. A mixed fleet of Reserved Instances scheduled between 9 a.m. to 5 p.m. and On-\r\nDemand Instances used for processing data workloads when needed randomly",
      "answer": "A, B. Elastic Beanstalk can run Amazon EC2 instances and build queues with Amazon \r\nSQS."
    },
    {
      "question": "Which of the following operations can AWS Elastic Beanstalk do? (Select TWO.)\r\nA. Access an Amazon Simple Storage Service (Amazon S3) bucket\r\nB. Connect to an Amazon Relational Database Service (Amazon RDS) database\r\nC. Install agents for Amazon GuardDuty service\r\nD. Create and manage Amazon WorkSpaces",
      "answer": "A, B. Elastic Beanstalk can access Amazon S3 buckets and connect to Amazon RDS data-\r\nbases. It cannot install Amazon GuardDuty agents or create or manage Amazon WorkSpaces."
    },
    {
      "question": "Which service can be used to restrict access to AWS Elastic Beanstalk resources?\r\nA. AWS Config\r\nB. Amazon Relational Database Service (Amazon RDS)\r\nC. AWS Identity and Access Management (IAM)\r\nD. Amazon Simple Storage Service (Amazon S3)",
      "answer": "C. By using IAM policies, you can control access to resources attached to users, groups, \r\nand roles."
    },
    {
      "question": "Which AWS Identity and Access Management (IAM) entities are used when creating an \r\nenvironment? (Select TWO.)\r\nA. Federated role\r\nB. Service role\r\nC. Instance profile\r\nD. Profile role\r\nE. User name and access keys",
      "answer": "B, C. Elastic Beanstalk creates a service role to access AWS services and an instance role to \r\naccess instances."
    },
    {
      "question": "Which of the following describes how customers are charged for AWS Elastic Beanstalk?\r\nA. A monthly fee based on an hourly rate for use.\r\nB. A one-time upfront cost for each environment running.\r\nC. No additional charges.\r\nD. A fee is charged only when scaling to support traffic changes.",
      "answer": "C. Elastic Beanstalk runs at no additional charge. You incur charges only for services \r\ndeployed."
    },
    {
      "question": "Which account is billed for user-accessed AWS resources allocated by AWS Elastic \r\nBeanstalk?\r\nA. The account running the services\r\nB. The cross-account able to access the shared services\r\nC. The cross-account with the Amazon Simple Storage Service (Amazon S3) bucket hold-\r\ning a downloaded copy of the code artifact\r\nD. All accounts involved",
      "answer": "D. Charges are incurred for all accounts that use the allocated resources."
    },
    {
      "question": "What can you not do to an Amazon Relational Database Service (Amazon RDS) instance \r\nwith AWS Elastic Beanstalk?\r\nA. Create a database connection.\r\nB. Create a supported Oracle edition.\r\nC. Retain a database instance despite the deletion of the environment’s database.\r\nD. Create a snapshot of the existing database (before deletion).\r\n",
      "answer": "C. An existing Amazon RDS instance is deleted if the environment is deleted. There is no \r\nauto-retention of the database instance. You must create a snapshot to retain the data and \r\nto restore the database.\r\n898 Appendix ■ Answers to Review Questions"
    }
  ],
  "Chapter 7": [
    {
      "question": "You have two AWS CodeDeploy applications that deploy to the same Amazon EC2 Auto \r\nScaling group. The first deploys an e-commerce app, while the second deploys custom \r\nadministration software. You are attempting to deploy an update to one application but \r\ncannot do so because another deployment is already in progress. You do not see any \r\ninstances undergoing deployment at this time. What could be the cause of this?\r\nA. If both deployment groups reference the same Auto Scaling group, a failure of the  \r\nfirst group’s deployment can block the second until the deployment times out. Since the \r\ninstance that failed deployment has been terminated from the Auto Scaling group, the AWS \r\nCodeDeploy agent is unable to provide results to the service.\r\nB. The AWS CodeDeploy agent is not installed on the instances as part of the launch \r\nconfiguration user data script.\r\nC. If both deployment groups reference the same Auto Scaling group, a failure of the first \r\ngroup’s deployment can block the second until the deployment times out. Since the instance \r\nthat failed deployment has been terminated from the Auto Scaling group, the AWS \r\nCodeDeploy service is unable to request status updates from the Amazon EC2 API.\r\nD. The AWS CodeDeploy agent is not installed in the Amazon Machine Image (AMI) \r\nbeing used.",
      "answer": "A. Options B and D are incorrect because the deployment is already in progress, and this \r\nwould not be possible if the AWS CodeDeploy agent had not been installed and running \r\nproperly. The CodeDeploy agent sends progress reports to the CodeDeploy service. The \r\nservice does not attempt to query instances directly, and the Amazon EC2 API does not \r\ninteract with instances at the operating system level. Thus, option C is incorrect, and option \r\nA is correct."
    },
    {
      "question": "If you specify a hook script in the ApplicationStop lifecycle event of an AWS CodeDeploy \r\nappspec.yml, will it run on the first deployment to your instance(s)?\r\nA. Yes\r\nB. No\r\nC. The ApplicationStop lifecycle event does not exist.\r\nD. It will run only if your application is running.",
      "answer": "B. Option B is correct because the ApplicationStop lifecycle event occurs before any \r\nnew deployment files download. For this reason, it will not run the first time a deployment \r\noccurs on an instance. Option C is incorrect, as this is a valid lifecycle event. Option A is \r\nincorrect. Option D is incorrect because lifecycle hooks are not aware of the current state of \r\nyour application. Lifecycle hook scripts execute any listed commands."
    },
    {
      "question": "If a single pipeline contains multiple sources, such as an AWS CodeCommit repository and \r\nan Amazon S3 archive, under what circumstances will the pipeline be triggered?\r\nA. When either a commit is pushed to the repository or the archive is updated, regardless \r\nof timing.\r\nB. When a commit is pushed to the repository and the archive is updated at the same time.\r\nC. When either a commit is pushed to the repository or the archive is updated, but not \r\nwhen both are updated at the same time.\r\nD. AWS CodePipeline does not support multiple sources in the same pipeline.",
      "answer": "A. Option B requires precise timing that would be overly burdensome to add to a CI/CD \r\nworkflow. Option C would not include edge cases where both sources are updated within \r\na small time period and would require separate release cadences for both sources. Option \r\nD is incorrect, as AWS CodePipeline supports multiple sources. When multiple sources are \r\nconfigured for the same pipeline, the pipeline will be triggered when any source is updated."
    },
    {
      "question": "If you want to implement a deployment pipeline that deploys both source files and large binary \r\nobjects to instance(s), how would you best achieve this while taking cost into consideration?\r\nA. Store both the source files and binary objects in AWS CodeCommit.\r\nB. Build the binary objects into the AMI of the instance(s) being deployed. Store the \r\nsource files in AWS CodeCommit.\r\nC. Store the source files in AWS CodeCommit. Store the binary objects in an Amazon S3 \r\narchive.\r\nD. Store the source files in AWS CodeCommit. Store the binary objects on an Amazon \r\nElastic Block Store (Amazon EBS) volume, taking snapshots of the volume whenever a \r\nnew one needs to be created.\r\nE. Store the source files in AWS CodeCommit. Store the binary objects in Amazon S3 and \r\naccess them from an Amazon CloudFront distribution.",
      "answer": "C. Option A is incorrect because storing large binary objects in a Git-based repository can \r\nincur massive storage requirements. Any time a binary object is modified in a repository, a \r\nnew copy is saved. Comparing cost to Amazon S3 storage, it is more expensive to take this \r\napproach. By building the binary objects into an Amazon Machine Image (AMI), you are \r\nrequired to create a new AMI any time changes are made to the objects; thus, option B is \r\nincorrect. Option D and E introduce unnecessary cost and complexity into the solution. By \r\nusing both an AWS CodeCommit repository and Amazon S3 archive, the lowest cost and \r\neasiest management is achieved."
    },
    {
      "question": "Your team is building a deployment pipeline to a sensitive application in your environment \r\nusing AWS CodeDeploy. The application consists of an Amazon EC2 Auto Scaling group \r\nof instances behind an Elastic Load Balancing load balancer. The nature of the application \r\nrequires 100 percent availability for both successful and failed deployments. The development \r\nteam want to deploy changes multiple times per day. \r\nHow would this be achieved at the lowest cost and with the fastest deployments?\r\nA. Rolling deployments with an additional batch\r\nB. Rolling deployments without an additional batch\r\nC. Blue/green deployments\r\nD. Immutable updates",
      "answer": "D. Option A is incorrect because rolling deployments without an additional batch would \r\nresult in less than 100 percent availability, as one batch of the original set of instances \r\nwould be taken out of circulation during the deployment process. Option B is incorrect \r\nbecause if you add an additional batch, it would ensure 100 percent availability at the low-\r\nest cost but would require a longer update process than replacing all instances at once. \r\nOption C is incorrect because, by default, blue/green deployments will leave the original \r\nenvironment intact, accruing charges until it is manually deleted. Option D is correct as \r\nimmutable updates would result in the fastest deployment for the lowest cost. In an immu-\r\ntable update, a new Auto Scaling group is created and registered with the load balancer. \r\nOnce health checks pass, the existing Auto Scaling group is terminated."
    },
    {
      "question": "What would cause an access denied error when attempting to download an archive file \r\nfrom Amazon S3 during a pipeline execution?\r\nA. Insufficient user permissions for the user initiating the pipeline\r\nB. Insufficient user permissions for the user uploading the Amazon S3 archive\r\nC. Insufficient role permissions for the Amazon S3 service role\r\nD. Insufficient role permissions for the AWS CodePipeline service role",
      "answer": "D. Option C is incorrect because Amazon S3 does not have a concept of service roles. \r\nWhen a pipeline is initiated, it is done in response either to a change in a source or when \r\na previous change is released by an authorized AWS IAM user or role. However, after the \r\npipeline has been initiated, the AWS CodePipeline service role is used to perform pipeline \r\nactions. Thus, options A and B are incorrect. Option D is correct, because the pipeline’s \r\nservice role requires permissions to download objects from Amazon S3."
    },
    {
      "question": "How do you output build artifacts from AWS CodeBuild to AWS CodePipeline?\r\nA. Write the outputs to STDOUT from the build container.\r\nB. Specify artifact files in the buildspec.yml configuration file.\r\nC. Upload the files to Amazon S3 from the build environment.\r\nD. Output artifacts are not supported with AWS CodeBuild.",
      "answer": "B. Option A is incorrect because this output is used only in the CodeBuild console. Option \r\nD is incorrect because CodeBuild natively supports this functionality. Though option C \r\nwould technically work, CodeBuild supports output artifacts in the buildspec.yml speci-\r\nfication. The BuildSpec includes a files directive to indicate any files from the build envi-\r\nronment that will be passed as output artifacts. Thus, option B is correct."
    },
    {
      "question": "What would be the most secure means of providing secrets to an AWS CodeBuild \r\nenvironment?\r\nA. Create a custom build environment with the secrets included in configuration files.\r\nB. Upload the secrets to Amazon S3 and download the object when the build job runs. \r\nProtect the bucket and object with an appropriate bucket policy.\r\nC. Save the secrets in AWS Systems Manager Parameter Store and query them as needed. \r\nEncrypt the secrets with an AWS Key Management Service (AWS KMS) key. Include \r\nappropriate AWS KMS permissions to your build environment’s IAM role.\r\nD. Include the secrets in the source repository or archive.",
      "answer": "C. Option A is incorrect because a custom build environment would expose the secrets \r\nto any user able to create new build jobs using the same environment. Option B is also \r\nincorrect. Though uploading the secrets to Amazon S3 would provide some protection, \r\nadministrators with Amazon S3 access may still be able to view the secrets. Option D is \r\nincorrect because AWS does not recommend storing sensitive information in source control \r\nrepositories, as it is easily viewed by anyone with access to the repository. Option D is cor-\r\nrect. By encrypting the secrets with AWS KMS and storing them in AWS Systems Manager \r\nParameter Store, you ensure that the keys are protected both at rest and in transit. Only \r\nAWS IAM users or roles with permissions to both the key and parameter store would have \r\naccess to the secrets."
    },
    {
      "question": "In which of the pipeline actions can you execute AWS Lambda functions?\r\nA. Invoke\r\nB. Deploy\r\nC. Build\r\nD. Approval\r\nE. Test",
      "answer": "A. Options B, C, D, and E are incorrect. AWS Lambda functions can execute as part of a \r\npipeline only with the Invoke action type."
    },
    {
      "question": "In what ways can pipeline actions be ordered in a stage? (Select TWO.)\r\nA. Series\r\nB. Parallel\r\nC. Stages support only one action each\r\nD. First-in-first-out (FIFO)\r\nE. Last-in-first-out (LIFO)",
      "answer": "A, B. Options D and E are incorrect because FIFO/LIFO are not valid pipeline action con-\r\nfigurations. Option C is incorrect because pipeline stages support multiple actions. Pipeline \r\nactions can be specified to occur both in series and in parallel within the same stage. Thus, \r\noptions A and B are correct."
    },
    {
      "question": "If you would like to delete an AWS CloudFormation stack before you deploy a new one in \r\nyour pipeline, what would be the correct set of actions?\r\nA. One action that specifies “Create or update a stack.”\r\nB. Two actions: the first specifies “Create or update a stack,” and the second specifies \r\n“Delete a stack.”\r\nC. Three actions: the first specifies “Delete a stack,” the second specifies “Create or \r\nupdate a stack,” and the third specifies “Replace a failed stack.”\r\nD. Two actions: the first specifies “Delete a stack,” and the second specifies “Create or \r\nupdate a stack.”",
      "answer": "D. Option A is incorrect because it will only create or update a stack, not delete the exist-\r\ning stack. Option B is incorrect because the desired actions are in the wrong order. Option \r\nC is incorrect because the final action, “Replace a failed stack,” is not needed. Option D is \r\ncorrect. Only two actions are required. First, the stack must be deleted. Second, the replace-\r\nment stack can be created. Unless otherwise required, however, both actions can be essen-\r\ntially accomplished by using one “Create or update a stack” action."
    },
    {
      "question": "How can you connect to an AWS CodeCommit repository without Git credentials?\r\nA. It is not possible.\r\nB. HTTPS\r\nC. SSH\r\nD. AWS CodeCommit credential helper",
      "answer": "D. Option A is incorrect. AWS CodeCommit is fully compatible with existing Git tools, \r\nand it also supports authentication with AWS Identity and Access Management (IAM) \r\ncredentials. Options B and C are incorrect. These are the only protocols over which you  \r\ncan interact with a repository. You can use the CodeCommit credential helper to  \r\nconvert an IAM access key and secret access key to valid Git credentials for SSH and \r\nHTTPS authentication. Thus, option D is correct."
    },
    {
      "question": "Of the following, which event cannot be used to generate notifications to an Amazon \r\nSimple Notification Service (SNS) topic from AWS CodeCommit without using a trigger?\r\nA. Pull Request Creation\r\nB. Commit Comments\r\nC. Commit Creation\r\nD. Pull Request Comments",
      "answer": "C. Options A, B, and D are all valid Amazon Simple Notification Service (Amazon SNS) \r\nnotification event sources for CodeCommit repositories. Option C is correct because Ama-\r\nzon SNS notifications cannot be configured to send when a commit is made to a repository."
    },
    {
      "question": "Which pipeline actions support AWS CodeBuild projects? (Select TWO.)\r\nA. Invoke\r\nB. Deploy\r\nC. Build\r\nD. Approval\r\nE. Test\r\n",
      "answer": "C, E. Options A, B, and D are incorrect because these action types do not support Code-\r\nBuild projects. Options C and E are correct because CodeBuild projects can be executed in \r\na pipeline as part of build and test actions."
    },
    {
      "question": "Can data passed to build projects using environment variables be encrypted or protected?\r\nA. Yes, this is supported natively by AWS CodeBuild.\r\nB. No, it is not supported.\r\nC. No, but this can be enabled in the console.\r\nD. No, but this can be supported using other AWS products and services.",
      "answer": "D. Environment variables in CodeBuild projects are not encrypted and are visible using the \r\nCodeBuild API. Thus, options A, B, and C are incorrect. If you need to pass sensitive infor-\r\nmation to build containers, use Systems Manager Parameter Store instead. Thus, option D \r\nis correct.\r\n900 Appendix ■ Answers to Review Questions"
    },
    {
      "question": "What is the only deployment type supported by on-premises instances?\r\nA. In-place\r\nB. Blue/green\r\nC. Immutable\r\nD. Progressive",
      "answer": "A. Because AWS does not have the ability to create or destroy infrastructure in customer \r\ndata centers, options B, C, and D are incorrect. Option A is correct because on-premises \r\ninstances support only in-place deployments."
    },
    {
      "question": "If your AWS CodeDeploy configuration includes creation of a file, nginx.conf, but the \r\nfile already exists on the server (prior to the use of AWS CodeDeploy), what is the default \r\nbehavior that will occur during deployment?\r\nA. The file will be replaced.\r\nB. The file will be renamed nginx.conf.bak, and the new file will be created.\r\nC. The deployment will fail.\r\nD. The deployment will continue, but the file will not be modified.",
      "answer": "C. Options A and B are incorrect because AWS CodeDeploy will not modify files on \r\nan instance that were not created by a deployment. Option D is incorrect because this \r\napproach could result in failed deployments because of missing settings in your configura-\r\ntion file. Option C is correct. By default, CodeDeploy will not remove files that it does not \r\nmanage. This is maintained as a list of files on the instance."
    },
    {
      "question": "How does AWS Lambda support in-place deployments?\r\nA. Function versions are overwritten during the deployment.\r\nB. New function versions are created, and then version numbers are switched.\r\nC. AWS Lambda does not support in-place deployments.\r\nD. Function aliases are overwritten during the deployment.",
      "answer": "C. Option A is incorrect because function versions cannot be modified after they have been \r\npublished. Option B is also incorrect because function version numbers cannot be changed. \r\nAliases can be used to point to different function versions; however, the alias itself can-\r\nnot be overwritten (it is a pointer to a function version). Thus, option D is incorrect. AWS \r\nLambda does not support in-place deployments. This is because, after a function version \r\nhas been published, it cannot be updated. Option C is correct."
    },
    {
      "question": "What is the minimum number of stages required by a pipeline in AWS CodePipeline?\r\nA. 0\r\nB. 1\r\nC. 2\r\nD. 3",
      "answer": "C. AWS CodePipeline requires that every pipeline contain a source stage and at least one \r\nbuild or deploy stage. Thus, the minimum number of stages is 2."
    },
    {
      "question": "If an instance is running low on storage, and you find that there are a large number of \r\ndeployment revisions stored by AWS CodeDeploy, what can be done to free up this space \r\npermanently?\r\nA. Delete the old revisions.\r\nB. Add an additional Amazon EBS volume.\r\nC. Configure the AWS CodeDeploy agent to store fewer revisions.\r\nD. Delete all of the revisions, and push all new code.\r\n",
      "answer": "C. Option A is not correct because deleting the old revisions will temporarily resolve the \r\nissue. However, future deployments will continue to consume disk space. The same reason-\r\ning applies to options B and D, which are also temporary solutions to the problem. The \r\nCodeDeploy agent configuration file includes a number of useful settings. Among these, a \r\nlimit can be set on how many revisions to store on an instance at any point in time. Thus, \r\noption C is correct."
    }
  ],
  "Chapter 8": [
    {
      "question": "Which of the AWS CloudFormation template sections is/are required?\r\nA. AWSTemplateFormatVersion\r\nB. Parameters\r\nC. Metadata\r\nD. Resources\r\nE. All of the above",
      "answer": "D. Only the Resources section of a template is required. If this section is omitted, AWS \r\nCloudFormation has no resources to manage. However, a template does not require Param-\r\neters, Metadata, or AWSTemplateFormatVersion. Thus, options A, B, C, and E are \r\nincorrect."
    },
    {
      "question": "You are writing an AWS CloudFormation template and would like to create an output \r\nvalue corresponding to your application’s website URL. The application is composed of \r\ntwo application servers in a private subnet behind an Elastic Load Balancing load balancer. \r\nThe application servers read from the Amazon Relational Database Service (Amazon RDS) \r\ndatabase instance. The logical IDs of the instances are AppServerA and AppServerB. The \r\nlogical IDs of the load balancer and database are AppLB and AppDB, respectively.\r\n\"Outputs\" : {\r\n    \"AppEndpoint\" : {\r\n        \"Description\" : \"URL to access the application\",\r\n        \"Value\" : \"Value to return\"\r\n    }\r\n}\r\nWhich code correctly completes the previous output declaration?\r\nA. { \"Fn::Join\": [ \"\", [ https://, { \"Ref\": \"AppLB\" }, \"/login.php\" ] ] }\r\nB. { \"Fn::Join\": [ \"\", [ https://, { \"Fn::GetAtt\": [ \"AppServerA\", \r\n\"PublicDNSName\" ] }, \"/login.php\" ] ] }\r\nC. { \"Fn::Join\": [ \"\", [ https://, { \"Ref\": [ \"AppLB\", \"DNSName\" ] },  \r\n\"/login.php\" ] ] }\r\nD. { \"Fn::Join\": [ \"\", [ https://, { \"Fn::GetAtt\": [ \"AppDB\", \"Endpoint \r\n.Address\" ] }, \"/login.php\" ] ] }\r\nE. { \"Fn::Join\": [ \"\", [ https://, { \"Fn::GetAtt\": [ \"AppLB\", \"DNSName\" ] }, \r\n\"/login.php\" ] ] }",
      "answer": "E. The return value of the Ref intrinsic function for an AWS::ElasticLoadBalancing:: \r\nLoadBalancer resource is the load balancer name, which is not valid in a URL, so option A \r\nis incorrect. Since the application server instances are in a private subnet, neither will have \r\na public DNS name; thus, option B is incorrect. Option C uses incorrect syntax for the Ref \r\nintrinsic function. Option D attempts to output a URL for the database instance. Thus, \r\noption E is correct."
    },
    {
      "question": "An AWS CloudFormation template you have written uses a CreationPolicy to ensure \r\nthat video transcoding instances launch and configure before the application server \r\ninstances so that they are available before users are able to access the website. However, \r\nyou are finding that the stack always reaches the creation policy’s timeout value before the \r\ntranscoding instances complete setup.\r\nWhy could this be? (Select THREE.)\r\nA. The user data script does not include a call to cfn-signal.\r\nB. The instance could not be launched because of account limits.\r\nC. The user data script fails before reaching the cfn-signal step.\r\nD. The instance cannot connect to the AWS CloudFormation endpoint when calling  \r\ncfn-signal.",
      "answer": "A, C, D. If account limits were preventing the launch of additional instances, the stack  \r\ncreation process would fail as soon as AWS CloudFormation attempts to launch the \r\ninstance (the Amazon EC2 API would return an error to AWS CloudFormation in  \r\nthis case). Thus, option B is incorrect. Any issues preventing the instance from calling  \r\ncfn-signal and sending a success/failure message to AWS CloudFormation would cause \r\nthe creation policy to time out. Thus, options A, C, and D are correct answers."
    },
    {
      "question": "When you attempt to update an Amazon Relational Database Service (Amazon RDS) \r\ninstance in your AWS CloudFormation stack, you experience a Resource failed to \r\nstabilize error, which causes the stack to roll back any changes you attempted.\r\nWhat might be the cause of this error, and how could it be resolved?\r\nA. The database is corrupted and cannot be updated. Take a snapshot of the database, \r\nand use it to create a replacement.\r\nB. The database took too long to update. Remove the database from the AWS \r\nCloudFormation stack by applying a DeletionPolicy of Retain, and manage the \r\nstack using the Amazon RDS console or AWS CLI.\r\nC. The database took too long to update, and the session credentials used by AWS \r\nCloudFormation timed out. Use a service role to perform the update.\r\nD. You have attempted to perform an update that is not supported by Amazon RDS. \r\nReview the specification documentation and attempt a valid update.\r\nE. I/O has not been halted on the database before performing the update, and AWS \r\nCloudFormation timed out waiting for database transactions to halt. Temporarily \r\nblock I/O and attempt the update again.",
      "answer": "C. Option A is incorrect because AWS CloudFormation does not monitor the status of your \r\ndatabase and would not be able to determine whether the database is corrupted. It also \r\ndoes not track whether there are currently running transactions before attempting updates. \r\nThus, option E is incorrect. If an invalid update is submitted, the stack generates an error \r\nmessage when attempting the database update. Thus, option D is incorrect. Though option \r\nB would work, it is not needed to remove the database from the stack and manage it \r\nseparately. Option C is correct because an AWS CloudFormation service role extends the \r\ndefault timeout value for stack actions to allow you to manage resources with longer update \r\nperiods."
    },
    {
      "question": "A custom resource associated with AWS Lambda in your stack creates successfully; \r\nhowever, it attempts to update the resource result in the failure message Custom Resource \r\nfailed to stabilize in the expected time. After you add a service role to extend the \r\ntimeout duration, the issue still persists.\r\nWhat may also be the cause of this error?\r\nA. The custom resource defined a function for handling the CREATE action but did not do \r\nthe same for the UPDATE action; thus, a success or failure signal was not sent to AWS \r\nCloudFormation.\r\nB. The service role does not have appropriate permissions to invoke the custom resource \r\nfunction.\r\nC. The custom resource function no longer exists.\r\nD. All of the above.",
      "answer": "A. Custom resource function permissions are obtained by a function execution role, not \r\nthe service role invoking the stack update; thus, option B is incorrect. When the AWS \r\nLambda function corresponding to a custom resource no longer exists, the custom resource \r\nwill fail to update immediately; thus, option C is incorrect. However, if the custom resource \r\nfunction is executed but does not provide a response to the AWS CloudFormation service \r\nendpoint, the resource times out with the aforementioned error. Thus, option A is correct."
    },
    {
      "question": "After you deploy an AWS Serverless Application Model (AWS SAM) template to AWS \r\nCloudFormation, can you view the original template? Why or why not?\r\nA. No, after the template is submitted and the AWS::Serverless transform is executed, \r\nan AWS CloudFormation-supported template is generated.\r\nB. Yes, the original template is saved and accessible using the get-stack-template AWS \r\nCLI command.\r\nC. Yes, it is saved in the Amazon Simple Storage Service (Amazon S3) bucket created by \r\nAWS CloudFormation for AWS SAM templates.\r\nD. No, AWS CloudFormation does not retain processed templates.\r\n",
      "answer": "A. AWS CloudFormation processes transformations by creating a change set, which  \r\ngenerates an AWS CloudFormation supported template. Without the AWS::Serverless  \r\ntransform, AWS CloudFormation cannot process the AWS SAM template. For any stack \r\nin your account, the current template can be downloaded using the get-stack-template \r\nAWS CLI command. This command will return templates as processed by AWS CloudFor-\r\nmation; thus, option B is incorrect. Option C is also incorrect, because the original template \r\nis not saved before executing the transform. Option D is also incorrect, as AWS CloudFor-\r\nmation saves the current template for all stacks."
    },
    {
      "question": "When defining an AWS Serverless Application Model (AWS SAM) template, how can you \r\ncreate an Amazon API Gateway as part of the stack?\r\nA. By defining an AWS::ApiGateway::RestApi resource and any associated \r\nAWS::ApiGateway::Method resources\r\nB. One will be created automatically for you whenever AWS::Serverless::Function \r\nresources are declared with one or more Events.\r\nC. By defining an AWS::Serverless::Api and providing an inline or external Swagger \r\ndefinition\r\nD. AWS::ApiGateway::RestApi resources are not supported in AWS SAM templates.\r\nE. A, B, and C",
      "answer": "E. AWS SAM supports other AWS CloudFormation resources, and it is not limited to defin-\r\ning only AWS::Serverless::* resource types; thus, option D is incorrect, and option A is \r\ncorrect. However, the AWS::Serverless transform will not automatically associate server-\r\nless functions with AWS::ApiGateway::RestApi resources. The transform will automati-\r\ncally associate any functions with the serverless API being declared, or it will create a new \r\none when the transform is executed. Thus, option B is also correct. Option C is also correct \r\nbecause AWS Serverless also supports Swagger definitions to outline the endpoints of your \r\nOpenAPI specification."
    },
    {
      "question": "Which of these helper scripts performs updates to OS configuration when an AWS \r\nCloudFormation stack updates?\r\nA. cfn-hup\r\nB. cfn-init\r\nC. cfn-signal\r\nD. cfn-update",
      "answer": "A. The cfn-init helper script is used to define which packages, files, and other configura-\r\ntions will be performed when an instance is first launched. The cfn-signal helper script is \r\nused to signal back to AWS CloudFormation when a resource creation or update has com-\r\npleted, so options B and C are incorrect. Option D is incorrect because cfn-update, is not \r\na valid helper script. The cfn-hup helper script performs updates on an instance when its \r\nparent stack is updated. Thus, option A is correct."
    },
    {
      "question": "Which of these options allows you to specify a required number of signals to mark the \r\nresource as CREATE_COMPLETE?\r\nA. Wait Condition\r\nB. Wait Condition Handler\r\nC. CreationPolicy\r\nD. WaitCount",
      "answer": "C. Wait conditions accept only one signal and will not track additional signals from the \r\nsame resource; thus, options A and B are incorrect. WaitCount is an invalid option type, so \r\noption D is incorrect. Option C is correct because creation policies enable you to specify a \r\ncount and timeout.\r\n902 Appendix ■ Answers to Review Questions"
    },
    {
      "question": "How would you preview the changes a stack update will make without affecting any \r\nresources in your account?\r\nA. Create a change set.\r\nB. Perform the stack update, and then manually roll back.\r\nC. Perform the stack update on a test stack.\r\nD. Do a manual diff of both templates.",
      "answer": "A. Options B and C will affect resources in your account. Option D would let you see the \r\nsyntax differences between two template versions, but this does not indicate what type of \r\nupdates will happen on the resources themselves. Thus, option D is incorrect. Change sets \r\ncreate previews of infrastructure changes without actually executing them. After reviewing \r\nthe changes that will be performed, the change set can be executed on the target stack."
    },
    {
      "question": "How would you access a property of a resource created in a nested stack?\r\nA. This cannot be done.\r\nB. In the child stack, declare the resource property as a stack output. In the parent \r\nstack, use Fn::GetAtt and pass in two parameters, the child stack logical ID and \r\nOutputs.NestedStackOutputName.\r\nC. In the child stack, export the resource property. In the parent stack, import the \r\nexported value.\r\nD. Use the cross-stack references.",
      "answer": "B. Option A is incorrect, as this is a supported feature of nested stacks. Option C creates a \r\ncircular dependency between the parent and child stacks (the parent stack needs to import \r\nthe value from the child stack, which cannot be created until the parent begins creation). \r\nOption D is incorrect because cross-stack references are not possible without exporting and \r\nimporting outputs. Option B uses intrinsic functions to access resource properties in the \r\nsame manner as any other stack resource."
    },
    {
      "question": "By default, with what permissions will AWS CloudFormation stack operations perform?\r\nA. Full administrator\r\nB. The permissions of the user performing the operation\r\nC. The AWS CloudFormation service role\r\nD. The AWS CloudFormation does not use permissions",
      "answer": "B. AWS CloudFormation does not assume full administrative control on your account, and \r\nit requires permissions to interact with resources you own. AWS CloudFormation can oper-\r\nate using a service role; however, this must be explicitly passed as part of the stack opera-\r\ntion. Otherwise, it will execute with the same permissions as the user performing the stack \r\noperation. Thus, option B is the correct answer."
    },
    {
      "question": "An AWS CloudFormation template declares two resources: an AWS Lambda function and \r\nan Amazon DynamoDB table. The function code is declared inline as part of the template \r\nand references the table. In what order will AWS CloudFormation provision the two \r\nresources?\r\nA. Amazon DynamoDB table, AWS Lambda function\r\nB. AWS Lambda function, Amazon DynamoDB table\r\nC. This cannot be determined ahead of time.\r\nD. This depends on the template.",
      "answer": "C. Because the reference to the Amazon DynamoDB table is made as part of an arbitrary \r\nstring (the function code), AWS CloudFormation does not recognize this as a dependency \r\nbetween resources. To prevent any potential errors, you would need to declare explicitly \r\nthat the function depends on the table. Thus, option C is correct."
    },
    {
      "question": "Which occurs during a replacing update?\r\nA. The resource becomes unavailable.\r\nB. The resource physical ID changes.\r\nC. A new resource is created.\r\nD. The original resource is deleted during the cleanup phase.\r\nE. All of the above",
      "answer": "E. Replacing updates results in the deletion of the original resource and the creation of a \r\nreplacement. AWS CloudFormation creates the replacement first with a new physical ID \r\nand verifies it before deleting the original. Because of this, option E is correct (all of the \r\nabove)."
    },
    {
      "question": "Which of the update types results in resource downtime? (Select TWO.)\r\nA. Update with No Interruption\r\nB. Update with Some Interruption\r\nC. Replacing Update\r\nD. Update with No Data\r\nE. Static Update",
      "answer": "B, C. Option A is incorrect, as it states that no interruption will occur. Options D and E \r\nare not valid update types. Replacing updates delete the original resource and provision \r\na replacement. Updates with some interruption have resource downtime, but the original \r\nresource is not replaced. Thus, options B and C are correct."
    },
    {
      "question": "What must occur before a stack that exports an output can be deleted?\r\nA. Any stacks importing the exported value must remove the import.\r\nB. The export must be removed from the stack.\r\nC. Nothing is required.\r\nD. The stack must be deleted.",
      "answer": "A. The export does not need to be removed from the stack before it can be deleted, so \r\noption B is incorrect. Options C and D are also incorrect, as the stack does not need to be \r\ndeleted. However, the stack cannot be deleted until any other stacks that import the value \r\nremove the import. Thus, option A is correct."
    },
    {
      "question": "If an AWS CloudFormation stack is in UPDATE_IN_PROGRESS state, which of the states are \r\npossible transitions? (Select THREE.)\r\nA. UPDATE_ROLLBACK_COMPLETE\r\nB. UPDATE_FAILED\r\nC. UPDATE_ROLLBACK_FAILED\r\nD. UPDATE_COMPLETE\r\nE. UPDATE_COMPLETE_CLEANUP_IN_PROGRESS\r\n",
      "answer": "B, D, E. If a stack update fails for any reason, the next state would be UPDATE_ ROLLBACK_\r\nIN_PROGRESS, which must occur before the rollback fails or completes. A stack that is  \r\ncurrently updating can either complete the update, fail to update, or complete and clean up \r\nold resources. Thus, options B, D, and E are correct."
    },
    {
      "question": "What does it mean when an AWS CloudFormation stack is in  \r\nUPDATE_COMPLETE_CLEANUP_IN_PROGRESS state?\r\nA. The stack has failed to update, and it is removing newly created resources.\r\nB. The stack has successfully updated, and it is removing old resources.\r\nC. The stack has successfully updated, and it is removing new resources.\r\nD. The stack has failed to update, and it is removing old resources.",
      "answer": "B. Because the stack status shows the update has completed, you know that the update \r\ndid not fail. This means that options A and D are incorrect. When a stack updates and \r\nresources are created, they will not be deleted unless the update fails. Thus, option C is \r\nincorrect. Old resources that are no longer required are removed during the cleanup phase. \r\nThus, option B is correct."
    },
    {
      "question": "Which of the formats are valid for an AWS CloudFormation template? (Select TWO.)\r\nA. YAML\r\nB. XML\r\nC. JSON\r\nD. Markdown\r\nE. LaTeX",
      "answer": "A, C. AWS CloudFormation currently supports JSON and YAML template formats only. "
    },
    {
      "question": "What are some challenges to consider when using the AWS Command Line Interface \r\n(AWS CLI) or AWS software development kits (AWS SDKs) to provision and manage \r\ninfrastructure compared to AWS CloudFormation?\r\nA. Reduction of human error\r\nB. Repeatable infrastructure\r\nC. Reduced IAM permissions requirements\r\nD. Versionable infrastructure\r\nE. All of the above",
      "answer": "E. AWS CloudFormation provides a number of benefits over procedural scripting. The risk \r\nof human error is reduced because templates are validated by AWS CloudFormation before \r\ndeployment. Infrastructure is repeatable and versionable using the same process as applica-\r\ntion code development. Individual users provisioning infrastructure need a reduced scope of \r\npermissions when using AWS CloudFormation service roles. Thus, option E is correct."
    },
    {
      "question": "What does a service token represent in a custom resource declaration?\r\nA. The AWS service that receives the request\r\nB. The Amazon Simple Notification Service (Amazon SNS) or AWS Lambda resource \r\nAmazon Resource Name (ARN) that receives the request\r\nC. The on-premises server IP address that receives the request\r\nD. The type of action to take\r\nE. The commands to execute for the custom resource",
      "answer": "B. Option C is incorrect because, though on-premises servers can be part of a custom \r\nresource’s workflow, they do not receive requests directly. Options D and E are incorrect \r\nbecause specific actions are not declared in custom resource properties. Option A is incor-\r\nrect because AWS services themselves do not process custom resource requests. Specifically, \r\nAmazon SNS topics and AWS Lambda functions can act as recipients to custom resource \r\nrequests. Thus, option B is correct."
    },
    {
      "question": "You are creating a custom resource associated with AWS Lambda that will execute several \r\ndatabase functions in an Amazon Relational Database Service (Amazon RDS) database \r\ninstance. As part of this, the functions will return data you would like to use in other \r\nresources declared in your AWS CloudFormation template.\r\nHow would you best pass this data to the other resources declared in the template?\r\nA. Store the data in a JSON file in an Amazon Simple Storage Service (Amazon S3) \r\nbucket, and use the AWS Command Line Interface (AWS CLI) to download the object.\r\nB. Store the output data in AWS Systems Manager Parameter Store, and query the \r\nparameter store using the AWS CLI.\r\nC. Use custom resource outputs to declare the returned data as resource properties. Then, \r\nquery the properties using the Fn::GetAtt intrinsic function.\r\nD. This cannot be accomplished.\r\n",
      "answer": "C. Options A and B are incorrect because they would require interacting with other AWS \r\nservices using the AWS CLI. For certain situations, such as running arbitrary commands in \r\nAmazon EC2 instance user data scripts, this would work. However, not all resource types \r\nhave this ability. Option D is incorrect, as this is a built-in functionality of AWS CloudFor-\r\nmation. Option C is correct because any data that is declared in a custom resource response \r\nis accessible to the remainder of the template using the Fn::GetAtt intrinsic function."
    }
  ],
  "Chapter 9": [
    {
      "question": "Which of the following AWS OpsWorks Stacks limits cannot be raised?\r\nA. Maximum stacks per account, per region\r\nB. Maximum layers per stack\r\nC. Maximum instances per layer\r\nD. Maximum apps per stack\r\nE. None of the above",
      "answer": "E. You can raise all of the limits listed by submitting a limit increase request to AWS Support."
    },
    {
      "question": "After submitting changes to your cookbook repository, you notice that executing cook-\r\nbooks on your AWS OpsWorks instances does not result in any changes taking place, even \r\nthough the logs show successful Chef runs.\r\nWhat could be the cause of this?\r\nA. The instances are unable to connect to the cookbook repository or archive location \r\nbecause of networking or permissions errors.\r\nB. The AWS OpsWorks Stacks agent running on the instance is enforcing cookbook \r\ncaching, resulting in cached copies being used instead of the new versions.\r\nC. The version of the cookbook specified in the recipe list for the lifecycle event is \r\nincorrect.\r\nD. The custom cookbooks have not yet been downloaded to the instances.",
      "answer": "D. Option A is incorrect because instances do not attempt to download new cookbooks \r\nwhen performing Chef runs. Option B is incorrect because AWS OpsWorks Stacks does not \r\nhave a concept of cookbook caching. Option C is incorrect because lifecycle events do not \r\nallow you to specify cookbook versions. Option D is correct because after updating a cus-\r\ntom cookbook repository, any currently online instances will not automatically receive the \r\nupdated cookbooks. To upload the modified cookbooks to the instances, you must first run \r\nthe Update Custom Cookbooks command."
    },
    {
      "question": "When will an AWS OpsWorks Stacks instance register and deregister from an Elastic Load \r\nBalancing load balancer associated with the layer?\r\nA. Instances are registered or deregistered manually only.\r\nB. Instances will be registered when they enter an online state and are deregistered when \r\nthey leave an online state.\r\nC. As an administrator, you are responsible for including the registration and deregistration \r\nwithin your Chef recipes and assigning the recipes to the appropriate lifecycle event.\r\nD. Instances are registered when they are created and not deregistered until they are \r\nterminated.",
      "answer": "B. Options A, C, and D are incorrect because OpsWorks Stacks provides integration with \r\nElastic Load Balancing to handle automatic registration and deregistration. Option B is cor-\r\nrect as the Elastic Load Balancing layers for OpsWorks Stacks automatically register instances \r\nwhen they come online and deregister them when they move to a different state. You can also \r\nenable connection draining to prevent deregistration until any active sessions end."
    },
    {
      "question": "You have an Amazon ECS cluster that runs on a single service with one task. The cluster \r\ncurrently contains enough instances to support the containers you define in your task, with \r\nno additional compute resources to spare (other than those needed by the underlying OS \r\nand Docker). Currently the service is configured with a maximum in-service percentage of \r\n100 percent and a minimum of 100 percent. When you attempt to update the service, noth-\r\ning happens for an extended period of time, as the replacement task appears to be stuck as \r\nit launches.\r\nHow would you resolve this? (Select TWO.)\r\nA. The current configuration prevents new tasks from starting because of insufficient \r\nresources. Add enough instances to the cluster to support the additional task temporarily.\r\nB. The current configuration prevents new tasks from starting because of insufficient \r\nresources. Modify the configuration to have a maximum in-service percentage of 200 \r\npercent and a minimum of 0 percent.\r\nC. Configure the cluster to leverage an AWS Auto Scaling group and scale out additional \r\ncluster instances when CPU Utilization is over 90 percent.\r\nD. Submit a new update to replace the one that appears to be failing.",
      "answer": "A, B. Option C is incorrect because changing the cluster capacity will not affect service \r\nscaling. Option D is incorrect because submitting a replacement will result in the same \r\nbehavior. If there are insufficient resources to launch replacement tasks when a service \r\nupdates, Amazon Elastic Container Service (Amazon ECS) will continue to attempt to \r\nlaunch the tasks until it is able to do so. If you increase the cluster size, additional resources \r\nadd to the pool to allow the new task to start. After it has done so, the old task will termi-\r\nnate. After it terminates, the cluster can scale back to its original size. If the downtime of \r\nthis service does not concern you, set the minimum in-service percentage to 0 percent to \r\nallow Amazon ECS to terminate the currently running task before it launches the new one. \r\nThus, options A and B are correct.\r\n904 Appendix ■ Answers to Review Questions"
    },
    {
      "question": "Which party is responsible for patching and maintaining underlying clusters when you use \r\nthe AWS Fargate launch type?\r\nA. The customer\r\nB. Amazon Web Services (AWS)\r\nC. Docker\r\nD. Independent software vendors",
      "answer": "B. Options A, C, and D are incorrect because no other parties have access to the underly-\r\ning clusters in AWS Fargate. When you use the Fargate launch type, AWS provisions and \r\nmanages underlying cluster instances for your containers. You do not need to manage main-\r\ntenance and patching. Thus, option B is correct."
    },
    {
      "question": "Why should instances in a single AWS OpsWorks Stacks layer have the same functionality \r\nand purpose?\r\nA. Because all instances in a layer run the same recipes\r\nB. To keep the console clean\r\nC. To stop and start at the same time\r\nD. To all run configure lifecycle events at the same time",
      "answer": "A. Option B is incorrect, as this is a matter of personal preference. Option C is also incor-\r\nrect because instances can be stopped and started individually, not only in layers at a \r\ntime. Option D is incorrect because the configure lifecycle event runs on all instances in a \r\nstack, regardless of layer. Assigning recipes is performed at the layer level, meaning that all \r\ninstances in the same layer will run the same configuration code. Organizing instances into \r\nlayers based on purpose removes the need to add complex conditional logic. Thus, option A \r\nis correct."
    },
    {
      "question": "Where do instances in an AWS OpsWorks Stacks stack download custom cookbooks?\r\nA. The Chef Server\r\nB. They are included in the Amazon Machine Image (AMI).\r\nC. The custom cookbook repository\r\nD. Amazon Elastic Container Service (Amazon ECS)",
      "answer": "C. Option A is incorrect because AWS OpsWorks Stacks does not include a central Chef \r\nServer. Option B is incorrect because storing recipes as part of an AMI would introduce \r\nconsiderable complexity for regular recipe code updates. Option D is incorrect because \r\nAmazon EC2 is not a valid storage location for cookbooks. A custom cookbook repository \r\nlocation is configured for a stack. When instances in the stack are first launched, they will \r\ndownload cookbooks from this location and run them as part of lifecycle events. Thus, \r\noption C is correct."
    },
    {
      "question": "How would you migrate an Amazon Relational Database Service (Amazon RDS) layer \r\nbetween two stacks in the same region?\r\nA. Supply the connection information to the second stack as custom JSON to ensure that \r\nthe instances can connect. Remove the Amazon RDS layer from the first stack. Add the \r\nAmazon RDS layer to the second stack. Remove the connection custom JSON.\r\nB. Add the Amazon RDS layer to the second stack and remove it from the first.\r\nC. Create a new database instance, migrate data to the new instance, and associate it with \r\nthe second stack using an Amazon RDS layer.\r\nD. This is not possible.",
      "answer": "A. Option B is incorrect because you cannot associate a single Amazon RDS database \r\ninstance with multiple stacks at the same time. Option C is incorrect because this approach \r\nwould require manual snapshotting and data migration that is not necessary. Option D \r\nis incorrect. Migration of database instances between stacks is a common workflow. To \r\nmigrate an Amazon RDS layer, you must remove it from the first layer before you add it to \r\nthe second. Thus, option A is correct."
    },
    {
      "question": "Which AWS OpsWorks Stacks instance type would you use for predictable increases in traf-\r\nfic or workload for a stack?\r\nA. 24/7\r\nB. Load-based\r\nC. Time-based\r\nD. On demand",
      "answer": "C. Option A is incorrect because 24/7 instances are normally recommended for constant \r\ndemand. Option B is incorrect because load-based instances are recommended for variable, \r\nunpredictable demand changes. Option D is incorrect because On-Demand is an Ama-\r\nzon ECS instance type, not an OpsWorks Stacks instance type. You configure time-based \r\ninstances to start and stop on a specific schedule. AWS recommends this for a predictable \r\nincrease in workload throughout a day. Thus, option C is correct."
    },
    {
      "question": "Which AWS OpsWorks Stacks instance type would you use for random, unpredictable \r\nincreases in traffic or workload for a stack?\r\nA. 24/7\r\nB. Load-based\r\nC. Time-based\r\nD. Spot",
      "answer": "B. Option A is incorrect because 24/7 instances are normally recommended for constant \r\ndemand. Option C is incorrect because time-based instances are recommended for changes \r\nin load that are predictable over time. Option D is incorrect because Spot is an Amazon \r\nECS instance type, not an OpsWorks Stacks instance type. Option B is correct because \r\nload-based instances are recommended for unpredictable changes in demand."
    },
    {
      "question": "What component is responsible for stopping and starting containers on an Amazon Elastic \r\nContainer Service (Amazon ECS) cluster instance?\r\nA. The Amazon ECS agent running on the instance\r\nB. The Amazon ECS service role\r\nC. AWS Systems Manager\r\nD. The customer",
      "answer": "A. Option B is incorrect because the Amazon ECS service role is used to create and man-\r\nage AWS resources on behalf of the customer. Option C is incorrect because AWS Systems \r\nManager is not part of Amazon ECS. Option D is incorrect because Amazon ECS auto-\r\nmates the process of stopping and starting containers within a cluster. The Amazon ECS \r\nagent is responsible for all on-instance tasks such as downloading container images and \r\nstarting or stoping containers. Thus, option A is correct."
    },
    {
      "question": "What is Service-Oriented Architecture (SOA)?\r\nA. The use of multiple AWS services to decouple infrastructure components and achieve \r\nhigh availability\r\nB. A software design practice where applications divide into discrete components (ser-\r\nvices) that communicate with each other in such a way that individual services do not \r\nrely on one another for their successful operation\r\nC. Involves multiple teams to develop application components with no knowledge of other \r\nteams and their components\r\nD. Leasing services from different vendors instead of doing internal development",
      "answer": "B. Option A is incorrect. Though high availability is a tenet of SOA, it is not a requirement. \r\nOption C is incorrect because SOA does not define how development teams are organized. \r\nOption D is incorrect because SOA does not define what should or should not be procured \r\nfrom vendors. Service-oriented architecture involves using containers to implement discrete \r\napplication components separately from one another to ensure availability and durability of \r\neach component. Thus, option B is correct."
    },
    {
      "question": "How many containers can a single task definition describe?\r\nA. 1\r\nB. Up to 3\r\nC. Up to 5\r\nD. Up to 10",
      "answer": "D. A single task definition can describe up to 10 containers to launch at a time. To launch \r\nmore containers, you need to create multiple task definitions. Task definitions should group \r\ncontainers by similar purpose, lifecycle, or resource requirements. Thus, option D is correct."
    },
    {
      "question": "You have a web proxy application that you would like to deploy in containers with the use \r\nof Amazon Elastic Container Service (Amazon ECS). Typically, your application binds to \r\nport 80 on the instance on which it runs. How can you use an application load balancer to \r\nrun more than one proxy container on each instance in your cluster?\r\nA. Do not configure the container to bind to port 80. Instead, configure Application Load \r\nBalancing (ALB) with dynamic host port mapping so that a random port is bound. \r\nThe ALB will route traffic coming in on port 80 to the port on which the container is \r\nlistening.\r\nB. Configure a Port Address Translation (PAT) instance in Amazon Virtual Private Cloud \r\n(Amazon VPC).\r\nC. If the container binds to a specific port, only one copy can launch per instance.\r\nD. Configure a classic load balancer to use dynamic host port mapping.\r\n",
      "answer": "A. Option B is incorrect because PAT cannot be configured within your VPC (it must be \r\nconfigured using a proxy instance of some kind). Option C is incorrect because containers \r\ncan be configured to bind to a random port instead of a specific one. Dynamic host port \r\nmapping allows you to launch multiple copies of the same container listening on different \r\nports. Classic Load Balancers do not support dynamic host port mapping. Thus, option D \r\nis incorrect. Option A is correct because the Application Load Balancer is then responsible \r\nfor mapping requests on one port to each container’s specific port."
    },
    {
      "question": "Which Amazon Elastic Container Service (Amazon ECS) task placement policy ensures that \r\ntasks are distributed as much as possible in a single cluster?\r\nA. Spread\r\nB. Binpack\r\nC. Random\r\nD. Least Cost\r\n",
      "answer": "A. Options B and C are incorrect because they do not consider the Availability Zone of \r\neach cluster instance when placing tasks. Option D is incorrect because least cost is not a \r\nvalid placement policy. The spread policy distributes tasks across multiple availability zones \r\nand cluster instances. Thus, option A is correct."
    }
  ],
  "Chapter 10": [
    {
      "question": "You need to grant a user, who is outside your AWS account, access to an object in an  \r\nAmazon Simple Storage Service (Amazon S3) bucket. Which is the best way to provide \r\naccess?\r\nA. Create a role and assign that role to the user.\r\nB. Create a user ID within Identity and Access Management (IAM) and assign the user ID \r\na policy that allows access.\r\nC. Create a new AWS account, assign that user to the account, and then give the account \r\ncross-account access.\r\nD. Have the user create a user ID using a third-party identity provider (IdP), and based on \r\nthat user ID, assign a policy that permits access.",
      "answer": "D. You need to use a third-party IdP as the confirmation of identity. Based on that confir-\r\nmation, a policy can be assigned. Option A is incorrect because roles cannot be assigned to \r\nusers outside of your account. Option B is incorrect because you cannot assign an IAM user \r\nID to a user that is external to AWS. Option C is incorrect because it makes provisioning an \r\nidentity a manual process."
    },
    {
      "question": "Which of the following is the purpose of an identity provider (IdP)?\r\nA. To control access to applications\r\nB. To control access to the AWS infrastructure\r\nC. To minimize the opportunity to assign the incorrect policy\r\nD. To answer the question “Who are you?”",
      "answer": "D. An identity provider (IdP) answers the question “Who are you?” Based on this answer, \r\npolicies are assigned. Those policies control the level of access to the AWS infrastructure \r\nand applications (if using AWS for managed services).\r\nOption A is incorrect; it is one of the functions of a service provider—to control access to \r\napplications. Option B is incorrect; policies are used to control access to APIs, which is how \r\naccess to the AWS infrastructure is controlled. Option C is incorrect; identity providers do \r\nno error checking on policy assignment."
    },
    {
      "question": "Which of the following is the best way to minimize misuse of AWS credentials?\r\nA. Set up multi-factor authentication (MFA).\r\nB. Embed the credentials in the bastion host and control access to the bastion host.\r\nC. Put a condition on all of your policies that allows execution only from your corporate \r\nIP range.\r\nD. Make sure that you have a limited number of credentials and limit the number of \r\npeople that can use them.",
      "answer": "A. Where possible, using multi-factor authentication (MFA) minimizes the impact of lost \r\nor compromised credentials. Option B is incorrect in that embedding credentials is both \r\na security risk and makes credential administration much more difficult. Option C would \r\ndecrease the opportunity for misuse. It would not address any misuse that was a result of \r\ninternal users. Option D is a good step but not as secure as option A.\r\n906 Appendix ■ Answers to Review Questions"
    },
    {
      "question": "Which of the following is not a valid identity provider (IdP) for Amazon Cognito?\r\nA. Google\r\nB. Microsoft Active Directory\r\nC. Your own identity store\r\nD. A Security Assertion Markup Language (SAML) 1.0–based IdP",
      "answer": "D. If you want to use Security Assertion Markup Language (SAML) as an identity provider \r\n(IdP), use SAML 2.0. With Amazon Cognito, you can use Google (option A), Microsoft \r\nActive Directory (option B), and your own identity store (option C) as identity providers."
    },
    {
      "question": "Which of the following is one benefit of using AWS as an identity provider (IdP) to access \r\nnon-AWS resources?\r\nA. AWS cannot be used as an IdP for non-AWS services.\r\nB. Using AWS as an IdP allows you to use Amazon CloudWatch to monitor activity.\r\nC. Using AWS as an IdP allows you to use AWS CloudTrail to audit who is using the \r\nservice.\r\nD. Using AWS as an IdP allows you to assign policies to non-AWS resources.\r\n",
      "answer": "C. By using AWS Cloud services, such as Amazon Cognito, you are able to view the API \r\ncalls in AWS CloudTrail. Amazon CloudWatch Logs are generated if you are using Amazon \r\nCognito to control access to AWS resources. Option A is incorrect as AWS can act as an \r\nIdP for non-AWS services. Option B is incorrect in that Amazon CloudWatch allows you to \r\nmonitor the creation and modification of identity pools. It will not show activity. Option D \r\nis incorrect because the service provider assigns the policies, not the identity provider (IdP)."
    },
    {
      "question": "Which of the following are benefits from using the Active Directory Connector (AD  \r\nConnector)? (Select TWO.)\r\nA. Easy setup\r\nB. Ability to connect to multiple Active Directory domains with a single connection\r\nC. Ability to configure changes to Active Directory on your existing Active Directory  \r\nconsole\r\nD. Ability to support authentication to non-AWS services",
      "answer": "A, C. AD Connecter is easy to set up, and you continue to use the existing AD console to \r\ndo configuration changes on Active Directory. Option B is incorrect because you cannot \r\nconnect to multiple Active Directory domains with AD Connector, only a single one. AD \r\nConnector requires a one-to-one relationship with your on-premises domains. You can  \r\nuse AD Connector for AWS-created applications and services. Option D is incorrect \r\nbecause AD Connector is used to support AWS services."
    },
    {
      "question": "Which of the following is a prerequisite for using AWS Single Sign-On (AWS SSO)?\r\nA. Set up AWS Organizations and enable all features.\r\nB. Make sure that your identity provider (IdP) is Security Assertion Markup Language \r\n(SAML) 2.0 compatible.\r\nC. Deploy AWS Simple Active Directory (Simple AD).\r\nD. Deploy Amazon Cognito.",
      "answer": "A. To use AWS Single Sign-On (AWS SSO), you must set up AWS Organizations Service \r\nand enable all the features. AWS SSO uses Microsoft Active Directory (either AWS Man-\r\naged Microsoft Active Directory or Active Directory Connector [AD Connector] but \r\nnot Simple Active Directory). AWS SSO does not support Amazon Cognito. Option B is \r\nincorrect because AWS SSO does not use SAML. Options C and D are incorrect because \r\nyou do not need to deploy either Simple AD or Amazon Cognito as a prerequisite for \r\nusing AWS SSO."
    },
    {
      "question": "AWS Security Token Service (AWS STS) supports a number of different tokens.\r\n Which token would you use to establish a longer-term session?\r\nA. AssumeRole\r\nB. GetUserToken\r\nC. GetFederationToken\r\nD. GetSessionToken",
      "answer": "C. Option C is correct because GetFederationToken returns a set of temporary security \r\ncredentials (consisting of an access key ID, a secret access key, and a security token) for \r\na federated user. You call the GetFederationToken action using the long-term security \r\ncredentials of an IAM user. This is appropriate in contexts where those credentials can \r\nbe safely stored, usually in a server-based application. Option D is incorrect because \r\nGetSessionToken provides only temporary security credentials. Option A is incorrect \r\nbecause AssumeRole is shorter lived (the default is 60 minutes; can be extended to 720 min-\r\nutes). Options B and D are incorrect because GetUserToken and GetSessionToken are  \r\nnonexistent APIs."
    },
    {
      "question": "Which of the following is not a service that AWS Managed Microsoft AD provides?\r\nA. Daily snapshots\r\nB. Ability to manage the Amazon Elastic Compute Cloud (Amazon EC2) instances that \r\nAWS Managed Microsoft AD is running on\r\nC. Monitoring\r\nD. Ability to sync with on-premises Active Directory",
      "answer": "B. Because it is a managed service, you are not able to access the Amazon EC2 instances \r\ndirectly running AWS Managed Microsoft AD. AWS Managed Microsoft AD provides for \r\ndaily snapshots, monitoring, and the ability to sync with an existing on-premises Active \r\nDirectory."
    },
    {
      "question": "You are using an existing RADIUS-based multi-factor authentication (MFA) infrastructure.\r\n Which AWS service is your best choice?\r\nA. Active Directory Connector (AD Connector)\r\nB. AWS Managed Microsoft AD\r\nC. Simple Active Directory (Simple AD)\r\nD. No AWS service would be suitable.\r\n",
      "answer": "A. Amazon Active Directory Connector (AD Connector) allows you to use your existing \r\nRADIUS-based multi-factor authentication (MFA) infrastructure to provide authentication."
    }
  ],
  "Chapter 11": [
    {
      "question": "When a user submits a build into the build system, you want to send an email to the user, \r\nacknowledging that you have received the build request, and start the build. To perform \r\nthese actions at the same time, what type of a state should you use?\r\nA. Choice\r\nB. Parallel\r\nC. Task\r\nD. Wait",
      "answer": "B. Option B is correct because a Parallel state enables you to execute several different \r\nexecution paths at the same time in parallel. This is useful if you have activities or tasks \r\nthat do not depend on each other and can execute in parallel. This can make your workflow \r\ncomplete faster. Option A is incorrect because it executes only one of the branches, not all. \r\nOption C is incorrect because it can execute one task, not multiple. Option D is incorrect \r\nbecause it waits and does not execute any tasks."
    },
    {
      "question": "Suppose that a queue has no consumers. The queue has a maximum message retention \r\nperiod of 14 days. After 14 days, what happens?\r\nA. After 14 days, the messages are deleted and move to the dead-letter queue.\r\nB. After 14 days, the messages are deleted and do not move to the dead-letter queue.\r\nC. After 14 days, the messages are not deleted.\r\nD. After 14 days, the messages become invisible.",
      "answer": "B. The messages move to the dead-letter queue if they have met the Maximum Receives \r\nparameter (the number of times that a message can be received before being sent to a dead-\r\nletter queue) and have not been deleted."
    },
    {
      "question": "What is size of an Amazon Simple Queue Service (Amazon SQS) message?\r\nA. 256 KB\r\nB. 128 KB\r\nC. 1 MB\r\nD. 5 MB",
      "answer": "A. Amazon Simple Queue Service (Amazon SQS) attributes supports 256 KB messages. \r\nRefer to Table 11.2, Table 11.3, and Table 11.4."
    },
    {
      "question": "You want to send a 1 GB file through Amazon Simple Queue Service (Amazon SQS). How \r\ncan you do this?\r\nA. This is not possible.\r\nB. Save the file in Amazon Simple Storage Service (Amazon S3) and then send a link to \r\nthe file on Amazon SQS.\r\nC. Use AWS Lambda to push the file.\r\nD. Bypass the log server so that it does not get overloaded.",
      "answer": "B. Option B is correct because to send a message larger than 256 KB, you use Amazon SQS \r\nto save the file in Amazon S3 and then send a link to the file on Amazon SQS. Option A \r\nis incorrect because using the technique in option B, this is possible. Option C is incorrect \r\nbecause AWS Lambda cannot push messages to Amazon SQS that exceed the size limit of \r\n256 KB. Option D is incorrect because it does not address the question."
    },
    {
      "question": "You want to design an application that sends a status email every morning to the system \r\nadministrators. Which option will work?\r\nA. Create an Amazon SQS queue. Subscribe all the administrators to this queue. Set up an \r\nAmazon CloudWatch event to send a message on a daily cron schedule into the Ama-\r\nzon SQS queue.\r\nB. Create an Amazon SNS topic. Subscribe all the administrators to this topic. Set up an \r\nAmazon CloudWatch event to send a message on a daily cron schedule to this topic.\r\nC. Create an Amazon SNS topic. Subscribe all the administrators to this topic. Set up \r\nan Amazon CloudWatch event to send a message on a daily cron schedule to an AWS \r\nLambda function that generates a summary and publishes it to this topic.\r\nD. Create an AWS Lambda function that sends out an email to the administrators every \r\nday directly with SMTP.",
      "answer": "C. Option C is correct if you need to send messages to other users. Create an Amazon \r\nSQS queue and subscribe all the administrators to this queue. Configure an Amazon \r\nCloudWatch event to send a message on a daily cron schedule into the Amazon SQS queue. \r\nOption A is not correct because Amazon SQS queues do not support subscriptions. Option \r\nB is not correct because the message is sent without any status information. Option D is not \r\ncorrect because AWS Lambda does not allow sending outgoing email messages on port 22. \r\nEmail servers use port 22 for outgoing messages. Port 22 is blocked on Lambda as an antis-\r\npam measure."
    },
    {
      "question": "What is the size of an Amazon Simple Notification Service (Amazon SNS) message?\r\nA. 256 KB\r\nB. 128 KB\r\nC. 1 MB\r\nD. 5 MB",
      "answer": "A. Amazon SNS supports the same attributes and parameters as Amazon SQS. Refer to \r\nTable 11.2, Table 11.3, and Table 11.4."
    },
    {
      "question": "You have an Amazon Kinesis data stream with one shard and one producer. How many \r\nconsumer applications can you consume from the stream?\r\nA. One consumer\r\nB. Two consumers\r\nC. Limitless number of consumers\r\nD. Limitless number of consumers as long as all consumers consume fewer than 2 MB and \r\nfive transactions per second",
      "answer": "D. Option D is correct because there is no limit on the number of consumers as long as \r\nthey stay within the capacity of the stream, which is based on the number of shards. For a \r\nsingle shard, the capacity is 2 MB of read or five transactions per second. Options A and B \r\nare incorrect because there is no limit on the number of consumers that can consume from \r\nthe stream. Option C is incorrect because together the consumers can consume only 2 MB \r\nper second or five transactions per second."
    },
    {
      "question": "A company has a website that sells books. It wants to find out which book is selling the \r\nmost in real time. Every time a book is purchased, it produces an event. What service can \r\nyou use to provide real-time analytics on the sales with a latency of 30 seconds?\r\nA. Amazon Simple Queue Service (Amazon SQS)\r\nB. Amazon Simple Notification Service (Amazon SNS)\r\nC. Amazon Kinesis Data Streams\r\nD. Amazon Kinesis Data Firehose",
      "answer": "C. Option C is correct because Amazon Kinesis Data Streams is a service for ingesting \r\nlarge amounts of data in real time and for performing real-time analytics on the data. \r\nOption A is not correct because you use Amazon SQS to ingest events, but it does not pro-\r\nvide a way to aggregate them in real time. Option B is incorrect because Amazon SNS is a \r\nnotification service that does not support ingesting. Option D is incorrect because Amazon \r\nKinesis Data Firehose provides analytics; however, it has a latency of at least 60 seconds.\r\n908 Appendix ■ Answers to Review Questions"
    },
    {
      "question": "A company sells books in the 50 states of the United States. It publishes each sale into an \r\nAmazon Kinesis data stream with two shards. For the partition key, it uses the two-letter \r\nabbreviation of the state, such as WA for Washington, WY for Wyoming, and so on. Which \r\nof the following statements is true?\r\nA. The records for Washington are all on the same shard.\r\nB. The records for both Washington and Wyoming are on the same shard.\r\nC. The records for Washington are on a different shard than the records for Wyoming.\r\nD. The records for Washington are evenly distributed between the two shards.\r\n",
      "answer": "A. Options B, C, and D are incorrect because there are no guarantees about where the \r\nrecords for Washington and Wyoming will be relative to each other. They could be on the \r\nsame shard, or they could be on different shards. Option A is correct because the records \r\nfor Washington will not be distributed across multiple shards."
    },
    {
      "question": "What are the options for Amazon Kinesis Data Streams producers?\r\nA. Amazon Kinesis Agent\r\nB. Amazon Kinesis Data Steams API\r\nC. Amazon Kinesis Producer Library (KPL)\r\nD. Open-Source Tools\r\nE. All of these are valid options.\r\n",
      "answer": "E. Option E is correct because all the options from A through D are correct. Options A, B, \r\nC, and D are all valid options for writing Amazon Kinesis Data Streams producers."
    }
  ],
  "Chapter 12": [
    {
      "question": "A company currently uses a serverless web application stack, which consists of Amazon API \r\nGateway, Amazon Simple Storage Service (Amazon S3), Amazon DynamoDB, and AWS \r\nLambda. They would like to make improvements to their AWS Lambda functions but do \r\nnot want to impact their production functions.\r\n How can they accomplish this?\r\nA. Create new AWS Lambda functions with a different name, and update resources to \r\npoint to the new functions when they are ready to test.\r\nB. Copy their AWS Lambda function to a new region where they can update their \r\nresources to the new region when ready.\r\nC. Create a new AWS account, and re-create all their serverless infrastructure for their \r\napplication testing.\r\nD. Publish the current version of their AWS Lambda function, and create an alias as PROD. \r\nThen, assign PROD to the current version number, update resources with the PROD alias \r\nARN, and create a new version of the updated AWS Lambda function and assign an \r\nalias of $DEV.",
      "answer": "D. Option D is correct because it enables the company to keep their existing AWS Lambda \r\nfunctions intact and create new versions of the AWS Lambda function. When they are \r\nready to update the Lambda function, they can assign the PROD alias to the new version. \r\nOption A is possible; however, this adds a lot of unnecessary work, because developers \r\nwould have to update all of their code everywhere. Option B is incorrect because moving \r\nregions would require moving all other services or introducing latency into the architecture, \r\nwhich is not the best option. Option C is possible; however, creating new AWS accounts for \r\neach application version is not a best practice, and it complicates the organization of such \r\naccounts unnecessarily."
    },
    {
      "question": "What is the maximum amount of memory that you can assign an AWS Lambda function?\r\nA. AWS runs the AWS Lambda function; it is a managed service, so you do not need to \r\nconfigure memory settings.\r\nB. 3008 MB\r\nC. 1000 MB\r\nD. 9008 MB",
      "answer": "B. At the time of this writing, the maximum amount of memory for a Lambda function is \r\n3008 MB."
    },
    {
      "question": "What is the default timeout value for an AWS Lambda function?\r\nA. 3 seconds\r\nB. 10 seconds\r\nC. 15 seconds\r\nD. 25 seconds",
      "answer": "A. At the time of this writing, the default timeout value for a Lambda function is  \r\n3 seconds. However, you can set this to as little as 1 second or as long as 300 seconds."
    },
    {
      "question": "A company uses a third-party service to send checks to its employees for payroll. The com-\r\npany is required to send the third-party service a JSON file with the person’s name and \r\nthe check amount. The company’s internal payroll application supports exporting only to \r\nCSVs, and it currently has cron jobs set up on their internal network to process these files. \r\nThe server that is processing the data is aging, and the company is concerned that it might \r\nfail in the future. It is also looking to have the AWS services perform the payroll function.\r\n What would be the best serverless option to accomplish this goal?\r\nA. Create an Amazon Elastic Compute Cloud (Amazon EC2) and the necessary cron job \r\nto process the file from CSV to JSON.\r\nB. Use AWS Import/Export to create a virtual machine (VM) image of the on-premises \r\nserver and upload the Amazon Machine Images (AMI) to AWS.\r\nC. Use AWS Lambda to process the file with Amazon Simple Storage Service  \r\n(Amazon S3).\r\nD. There is no way to process this file with AWS.",
      "answer": "C. Options A, B, and D are all viable answers; however, the question asks what is the best \r\nserverless option. Lambda is the only serverless option in this scenario; therefore, option C \r\nis the best answer."
    },
    {
      "question": "What is the maximum execution time allowed for an AWS Lambda function?\r\nA. 60 seconds\r\nB. 120 seconds\r\nC. 230 seconds\r\nD. 300 seconds",
      "answer": "D. At the time of this writing, the maximum execution time for a Lambda function is 300 \r\nseconds (5 minutes)."
    },
    {
      "question": "Which language is not supported for AWS Lambda functions?\r\nA. Ruby\r\nB. Python 3.6\r\nC. Node.js\r\nD. C# (.NET Core)",
      "answer": "A. At the time of this writing, Ruby is not supported for Lambda functions."
    },
    {
      "question": "How can you increase the limit of AWS Lambda concurrent executions?\r\nA. Use the Support Center page in the AWS Management Console to open a case and send \r\na Server Limit Increase request.\r\nB. AWS Lambda does not have any limits for concurrent executions.\r\nC. Send an email to limits@amazon.com with the subject “AWS Lambda Increase.”\r\nD. You cannot increase concurrent executions for AWS Lambda.",
      "answer": "A. At the time of this writing, the default limit for concurrent executions with Lambda is \r\nset to 1000. This is a soft limit that can be raised. To do this, you must open a case through \r\nthe AWS Support Center page and send a Server Limit Increase request."
    },
    {
      "question": "A company is receiving permission denied after its AWS Lambda function is invoked and \r\nexecutes and has a valid trust policy. After investigating, the company realizes that its AWS \r\nLambda function does not have access to download objects from Amazon Simple Storage \r\nService (Amazon S3).\r\n Which type of policy do you need to correct to give access to the AWS Lambda function?\r\nA. Function policy\r\nB. Trust policy\r\nC. Execution policy\r\nD. None of the above",
      "answer": "C. There are two types of policies with Lambda: a function policy and an execution policy, \r\nor AWS role. A function policy defines which AWS resources are allowed to invoke your \r\nfunction. The execution role defines which AWS resources your function can access. Here, \r\nthe function is invoked successfully, but the issue is that the Lambda function does not have \r\naccess to process objects inside Amazon S3. Option A is not correct because a function \r\npolicy is responsible for invoking or triggering the function; here, the function is invoked \r\nand executes properly. Option B is not correct, as the scenario states that the trust policy is \r\nvalid. The execution policy or AWS role is responsible for providing Lambda with access to \r\nother services; thus, the correct answer is option C."
    },
    {
      "question": "A company wants to be able to send event payloads to an Amazon Simple Queue Service \r\n(Amazon SQS) queue if the AWS Lambda function fails.\r\n Which of the following configuration options does the company need to be able to do this \r\nin AWS Lambda?\r\nA. Enable a dead-letter queue.\r\nB. Define an Amazon Virtual Private Cloud (Amazon VPC) network.\r\nC. Enable concurrency.\r\nD. AWS Lambda does not support such a feature.\r\n",
      "answer": "A. Option A is correct because Lambda automatically retries failed executions for asyn-\r\nchronous invocations. You can also configure Lambda to forward payloads that were not \r\nprocessed to a DLQ, which can be an Amazon SQS queue or Amazon SNS topic. Option B \r\nis incorrect because a VPC network is an AWS service that allows you to define your own \r\nnetwork in the AWS Cloud. Option C is incorrect because this is dealing with concurrency \r\nissues, and here you have no problems with Lambda concurrency. Additionally, concur-\r\nrency is enabled by default with Lambda. Option D is incorrect because Lambda does sup-\r\nport SQS."
    },
    {
      "question": "A company wants to be able to pass configuration settings as variables to their AWS \r\nLambda function at execution time.\r\n Which feature should the company use?\r\nA. Dead-letter queues\r\nB. AWS Lambda does not support such a feature.\r\nC. Environment variables\r\nD. None of the above\r\n",
      "answer": "C. Option C is correct because the environment variables enable you to pass settings \r\ndynamically to your function code and libraries without changing your code. Option A is \r\nnot correct, because dead-letter queries are used for events that could not be processed by \r\nLambda and need to be investigated later. Option B is not correct because it can be done. \r\nOption D is incorrect because this can be accomplished through environment variables."
    }
  ],
  "Chapter 13": [
    {
      "question": "Which templating engine can you use to deploy infrastructure inside of AWS that is built \r\nfor serverless technologies?\r\nA. AWS CloudFormation\r\nB. Ansible\r\nC. AWS OpsWorks for Automate Operations\r\nD. AWS Serverless Application Model (AWS SAM)",
      "answer": "D. Option A is incorrect. While AWS CloudFormation can help you provision infrastruc-\r\nture, AWS Serverless Application Model (AWS SAM) is optimized for deploying AWS \r\nserverless resources by making it easy to organize related components and resources that \r\noperate on a single stack; therefore, option A is not the best answer. Option C is incor-\r\nrect because AWS OpsWorks is managed by Puppet or Chef, which you can use to deploy \r\ninfrastructure. However, these are not the optimal answers given that you are specifically \r\nlooking for serverless technologies. The same is true for Ansible in option B. Option D is \r\ncorrect because AWS SAM is an open-source framework that you can use to build serverless \r\napplications on AWS."
    },
    {
      "question": "What option do you need to enable to call Amazon API Gateway from another server or \r\nservice?\r\nA. You do not need to enable any options. Amazon API Gateway is ready to use as soon \r\nas it’s deployed.\r\nB. Enable cross-origin resource sharing (CORS).\r\nC. Deploy a stage.\r\nD. Deploy a resource.",
      "answer": "B. CORS is responsible for allowing cross-site access to your APIs. Without it, you will \r\nnot be able to call the Amazon API Gateway service. You use a stage to deploy your API, \r\nand a resource is a typed object that is part of your API’s domain. Each resource may have \r\nan associated data model and relationships to other resources and can respond to different \r\nmethods. Option A is incorrect because you do need to enable CORS. Option B is cor-\r\nrect because CORS is responsible for allowing one server to call another server or service. \r\nFor more information on CORS, see: https://developer.mozilla.org/en-US/docs/\r\nWeb/HTTP/CORS. Option C is incorrect, as deploying a stage allows you to deploy your \r\nAPI. Option D is incorrect, as a resource is where you can define your API, but it is not yet \r\ndeployed to a stage and “live.”"
    },
    {
      "question": "A company is considering moving to the AWS serverless stack. What are two benefits of \r\nserverless stacks? (Select TWO.)\r\nA. No server management\r\nB. It costs less than Amazon Elastic Compute Cloud (Amazon EC2).\r\nC. Flexible scaling\r\nD. There are no benefits to serverless stacks.",
      "answer": "A, C. There are three benefits to serverless stacks: no server management, flexible scaling, \r\nand automated high availability. Costs vary case by case. For these reasons, option A and \r\noption C are the best answers."
    },
    {
      "question": "Can you create HTTP endpoints with Amazon API Gateway?\r\nA. Yes. You can create HTTP endpoints with Amazon API Gateway.\r\nB. No. API Gateway creates FTP endpoints.\r\nC. No. API Gateway only supports SSH endpoints.\r\nD. No. API Gateway is a secure service that only supports HTTPS.",
      "answer": "D. Option A is incorrect; API Gateway only supports HTTPS endpoints. Option B is \r\n incorrect because API Gateway does not support creating FTP endpoints. Option C  \r\nis incorrect; API Gateway does not support SSH endpoints. API Gateway only creates \r\nHTTPS endpoints.\r\n910 Appendix ■ Answers to Review Questions"
    },
    {
      "question": "A company is moving to a serverless application, using Amazon Simple Storage Service \r\n(Amazon S3), AWS Lambda, and Amazon DynamoDB. They are currently using Amazon \r\nCloudFront for their content delivery network (CDN) network. They are concerned that \r\nthey can no longer use Amazon CloudFront because they will have no Amazon Elastic \r\nCompute Cloud (Amazon EC2) instances running. Is their concern valid?\r\nA. Their concerns are valid: Amazon CloudFront only supports Amazon EC2.\r\nB. Their concerns are valid because all serverless applications are fully dynamic and  \r\ncontain no static information; thus, Amazon CloudFront does not support serverless \r\napplications.\r\nC. Their concerns are not valid. Amazon CloudFront supports serverless applications\r\nD. Their concerns are valid. Amazon CloudFront does support serverless applications; \r\nhowever, it does not support Amazon S3.",
      "answer": "C. Option A is incorrect because Amazon CloudFront supports a variety of sources,  \r\nincluding Amazon S3. Option B is incorrect, because serverless applications contain both \r\nstatic and dynamic data. Additionally, CloudFront supports both static and dynamic data. \r\nOption C is correct because CloudFront supports a variety of origins. For the serverless \r\nstack, it supports Amazon S3. Option D is incorrect because Amazon S3 is a valid origin \r\nfor CloudFront."
    },
    {
      "question": "Amazon Cognito Mobile SDK does not support which language/platform?\r\nA. iOS\r\nB. Android\r\nC. JavaScript\r\nD. All of these languages/platform are supported.",
      "answer": "D. Option A, option B, and option C are each not the only language/platform supported. \r\nOption D is correct because all of these languages/platforms are supported."
    },
    {
      "question": "Does Amazon Cognito support Short Message Service (SMS)–based multi-factor authenti-\r\ncation (MFA)?\r\nA. No. Amazon Cognito does not support SMS-based MFA.\r\nB. No. Amazon Cognito does not support SMS-based MFA; however, it does support \r\nMFA.\r\nC. Yes. Amazon Cognito does support SMS-based MFA.\r\nD. None of the above.",
      "answer": "C. Option C is correct because Amazon Cognito supports SMS-based MFA."
    },
    {
      "question": "Does Amazon Cognito support device tracking and remembering?\r\nA. Amazon Cognito does not support device tracking and remembering.\r\nB. Amazon Cognito supports device tracking but not remembering.\r\nC. Amazon Cognito supports device remembering but not tracking.\r\nD. Amazon Cognito supports device remembering and tracking.",
      "answer": "D. Options A, B, and C are incorrect because Amazon Cognito supports device tracking \r\nand remem bering."
    },
    {
      "question": "What is the property name that you use to connect an AWS Lambda function to the Amazon \r\nAPI Gateway inside of an AWS Serverless Application Model (AWS SAM) template?\r\nA. events\r\nB. handler\r\nC. context\r\nD. runtime",
      "answer": "A. Option A is correct because the events property allows you to assign Lambda to an \r\nevent source. Option B is incorrect because handler is the function handler in an Lambda \r\nfunction. Option C is incorrect because context is the context object for a Lambda func-\r\ntion. Option D is incorrect because runtime is the language that your Lambda function \r\nruns as."
    },
    {
      "question": "A company wants to use a serverless application to run its dynamic website that is currently \r\nrunning on Amazon Elastic Compute Cloud (Amazon EC2) and Elastic Load Balancing \r\n(ELB). Currently, the application uses HTML, CSS, and React, and the database is a \r\nNoSQL flavor. You are the advisor—is this possible?\r\nA. No. This is not possible, because there is no way to run React in AWS. React is a  \r\nFacebook technology.\r\nB. No. This is not possible, because you need an Amazon EC2 to run the web server.\r\nC. No. This is not possible, because there is no way to load balance a serverless  \r\napplication.\r\nD. Yes. This is possible; however, some refactoring will be required.\r\n",
      "answer": "D. Option A is incorrect. You can run React in an AWS service. Option B is incorrect. You \r\ncan run your web server with Amazon S3. With option C, you do not need to load balance \r\nLambda functions because Lambda scales automatically. Option D is correct. You can run \r\na fully dynamic website in a serverless fashion. You can also use JavaScript frameworks \r\nsuch as Angular and React. The NoSQL database may need to be refactored to run in  \r\nAmazon DynamoDB."
    }
  ],
  "Chapter 14": [
    {
      "question": "Which of the following is the maximum Amazon DynamoDB item size limit?\r\nA. 512 KB\r\nB. 400 KB\r\nC. 4 KB\r\nD. 1,024 KB",
      "answer": "B. Option B is correct because the maximum size of an item in an DynamoDB table is  \r\n400 KB. Option C is incorrect because 4 KB is the capacity of a strongly consistent read \r\nper second, or two eventually consistent reads per second, for an item up to 4 KB in size. \r\nOption D is incorrect because 1,024 KB is not the size limit of an DynamoDB item. The \r\nmaximum item size is 400 KB."
    },
    {
      "question": "Which of the following is true when using Amazon Simple Storage Service (Amazon S3)?\r\nA. Versioning is enabled on a bucket by default.\r\nB. The largest size of an object in an Amazon S3 bucket is 5 GB.\r\nC. Bucket names must be globally unique.\r\nD. Bucket names can be changed after they are created.",
      "answer": "C. Option C is correct because when creating a new bucket, the bucket name must be glob-\r\nally unique. Option A is incorrect because versioning is disabled by default. Option B is incor-\r\nrect because the maximum size for an object stored in Amazon S3 is 5 TB, not 5 GB. Option \r\nD is incorrect because you cannot change a bucket name after you have created the bucket."
    },
    {
      "question": "Which of the following is not a deciding factor when choosing an AWS Region for your \r\nbucket?\r\nA. Latency\r\nB. Storage class\r\nC. Cost\r\nD. Regulatory requirements",
      "answer": "B. Option B is correct because storage class is the only factor that is not considered when \r\ndetermining which region to choose. Option A is incorrect because latency is a factor \r\nwhen choosing a bucket region. Option C is incorrect because prices are different between \r\nregions; thus, you might consider cost when choosing a bucket region. Option D is incorrect \r\nbecause you may be required to store your data in a bucket in a particular region based on \r\nlegal requirements or compliance."
    },
    {
      "question": "Which of the following features can you use to protect your data at rest within Amazon \r\nDynamoDB?\r\nA. Fine-grained access controls\r\nB. Transport Layer Security (TLS) connections\r\nC. Server-side encryption provided by the DynamoDB service\r\nD. Client-side encryption",
      "answer": "C. Option C is correct because the recommended technique for protecting your table data \r\nat rest is the server-side encryption. Option A is incorrect because fine-grained access con-\r\ntrols are a mechanism for providing access to resources and API calls, but the mechanism is \r\nnot used to encrypt or protect data at rest. Option B is incorrect because TLS protects data \r\nin transit, not data at rest. Option D is incorrect because client-side encryption is applied to \r\ndata before it is transmitted from a user device to a server."
    },
    {
      "question": "You store your company’s critical data in Amazon Simple Storage Service (Amazon S3). \r\nThe data must be protected against accidental deletions or overwrites. How can this be \r\nachieved?\r\nA. Use a lifecycle policy to move the data to Amazon S3 Glacier.\r\nB. Enable MFA Delete on the bucket.\r\nC. Use a path-style URL.\r\nD. Enable versioning on the bucket.",
      "answer": "D. Option D is correct because versioning-enabled buckets enable you to recover objects \r\nfrom accidental deletion or overwrite. Option A is incorrect because lifecycle policies are \r\nused to transition data to a different storage class and do not protect objects against acci-\r\ndental overwrites or deletions. Option B is incorrect because enabling MFA Delete on the \r\nbucket requires an additional method of authentication before allowing a deletion. Option \r\nC is incorrect because using a path-style URL is unrelated to protecting overwrites or acci-\r\ndental deletions."
    },
    {
      "question": "How does Amazon Simple Storage Service (Amazon S3) object storage differ from block \r\nand file storage? (Select TWO.)\r\nA. Amazon S3 stores data in fixed blocks.\r\nB. Objects can be any size.\r\nC. Objects are stored in buckets.\r\nD. Objects contain both data and metadata.\r\n",
      "answer": "C, D. Options C and D are correct because Amazon S3 stores objects in buckets, and each \r\nobject that is stored in a bucket is made up of two parts: the object itself and the metadata. \r\nOption A is incorrect because Amazon S3 stores data as objects, not in fixed blocks. Option \r\nB is incorrect because the size limit of an object is 5 TB."
    },
    {
      "question": "What is the lifetime of data in an Amazon DynamoDB stream?\r\nA. 14 days\r\nB. 12 hours\r\nC. 24 hours\r\nD. 4 days",
      "answer": "C. Option C is correct because DynamoDB Streams captures a time-ordered sequence of \r\nitem-level modifications in any DynamoDB table, and the service stores this information in \r\na log for up to 24 hours. Options A, B, and D are incorrect because  \r\n24 hours is the maximum time that data persists on an Amazon DynamoDB stream."
    },
    {
      "question": "How many times does each stream record in Amazon DynamoDB Streams appear in the \r\nstream?\r\nA. Twice\r\nB. Once\r\nC. Three times\r\nD. This value can be configured.",
      "answer": "B. Option B is correct because DynamoDB Streams ensures that each stream record \r\nappears exactly once in the stream. Options A and C are incorrect because each stream \r\nrecord appears exactly once. Option D is incorrect because you cannot set the retention \r\nperiod."
    },
    {
      "question": "Versioning is a means of keeping multiple variants of an object in the same bucket. You \r\ncan use versioning to preserve, retrieve, and restore every version of every object stored in \r\nyour Amazon S3 bucket. With versioning, you can easily recover from both unintended \r\nuser actions and application failures. Which of the following is not a versioning state of a \r\nbucket?\r\nA. Versioning paused\r\nB. Versioning disabled\r\nC. Versioning suspended\r\nD. Versioning enabled",
      "answer": "A. Option A is correct because your bucket can be in only one of three versioning states: \r\nversioning-enabled, versioning-disabled, or versioning-suspended. Thus, versioning-paused \r\nis a state that is not a valid configuration. Options A, B, and C are incorrect—they are all \r\nvalid bucket states for versioning."
    },
    {
      "question": "Your team has built an application as a document management system that maintains meta-\r\ndata on millions of documents in a DynamoDB table. When a document is retrieved, you \r\nwant to display the metadata beside the document. Which DynamoDB operation can you \r\nuse to retrieve metadata attributes from a table?\r\nA. QueryTable\r\nB. UpdateTable\r\nC. Search\r\nD. Scan",
      "answer": "A. Option A is correct because QueryTable is the DynamoDB operation used to find items \r\nbased on primary key values. Option B is incorrect because UpdateTable is the DynamoDB \r\noperation used to modify the provisioned throughput settings, global secondary indexes,  \r\nor DynamoDB Streams settings for a given table. Option C is incorrect because DynamoDB \r\ndoes not have a Search operation. Option D is incorrect because Scan is the DynamoDB \r\noperation used to read every item in a table."
    },
    {
      "question": "Which of the following objects are good candidates to store in a cache? (Select THREE.)\r\nA. Session state\r\nB. Shopping cart\r\nC. Product catalog\r\nD. Bank account balance",
      "answer": "A, B, C. Option D is incorrect because when compared to the other options, a bank \r\nbalance is not likely to be stored in a cache; it is probably not data that is retrieved as \r\nfrequently as the others are fetched. Options A, B, and C are all better data candidates \r\nto cache because multiple users are more likely to access them repeatedly. Although, you \r\ncould also cache the bank account balance for shorter periods if the database query is not \r\nperforming well.\r\n912 Appendix ■ Answers to Review Questions"
    },
    {
      "question": "Which of the following cache engines does Amazon ElastiCache support? (Select TWO.)\r\nA. Redis\r\nB. MySQL\r\nC. Couchbase\r\nD. Memcached",
      "answer": "A, D. Options A and D are correct because Amazon ElastiCache supports both the Redis \r\nand Memcached open-source caching engines. Option B is incorrect because MySQL is \r\nnot a caching engine—it is a relational database engine. Option C is incorrect because \r\nCouchbase is a NoSQL database and not one of the caching engines that ElastiCache \r\nsupports."
    },
    {
      "question": "How many nodes can you add to an Amazon ElastiCache cluster that is running Redis?\r\nA. 100\r\nB. 5\r\nC. 20\r\nD. 1",
      "answer": "C. Option C is correct because the default limit is 20 nodes per cluster."
    },
    {
      "question": "What feature does Amazon ElastiCache provide?\r\nA. A highly available and fast indexing service for querying\r\nB. An Amazon Elastic Compute Cloud (Amazon EC2) instance with a large amount of \r\nmemory and CPU\r\nC. A managed in-memory caching service\r\nD. An Amazon EC2 instance with Redis and Memcached already installed",
      "answer": "C. Option C is correct because ElastiCache is a managed in-memory caching service. \r\nOption A is incorrect because the description aligns more closely to the Elasticsearch \r\nService. Option B is incorrect because this is not an accurate description of the ElastiCache \r\nservice. Option D is incorrect because, as a managed service, ElastiCache does not manage \r\nAmazon EC2 instances."
    },
    {
      "question": "When designing a highly available web solution using stateless web servers, which services \r\nare suitable for storing session-state data? (Select THREE.)\r\nA. Amazon CloudFront\r\nB. Amazon DynamoDB\r\nC. Amazon CloudWatch\r\nD. Amazon Elastic File System (Amazon EFS)\r\nE. Amazon ElastiCache\r\nF. Amazon Simple Queue Service (Amazon SQS)",
      "answer": "B, D, E. Option B is correct because DynamoDB is a NoSQL low-latency transactional \r\ndatabase that you can use to store state. Option D is correct because Amazon Elastic File \r\nSystem (Amazon EFS) is an elastic file system that you can also use to store state. Option E \r\nis correct because ElastiCache is an in-memory cache that is also a good solution for storing \r\nstate. Option A is incorrect because Amazon CloudFront is a content delivery network that \r\nis used more for object caching, not in-memory caching. Option C is incorrect because \r\nAmazon CloudWatch is a metric repository and does not provide any kind of user-accessible \r\nstorage. Option F is incorrect because Amazon SQS is used for exchanging messages."
    },
    {
      "question": "Which AWS database service is best suited for nonrelational databases?\r\nA. Amazon Simple Storage Service Glacier (Amazon S3 Glacier)\r\nB. Amazon Relational Database Service (Amazon RDS)\r\nC. Amazon DynamoDB\r\nD. Amazon Redshift",
      "answer": "C. Option C is correct because Amazon DynamoDB is a nonrelational database that deliv-\r\ners reliable performance at any scale. Option A is incorrect because Amazon S3 Glacier is \r\nfor data archiving and long-term backup. It is also an object store and not a database store. \r\nOption B is incorrect because Amazon RDS is designed for relational workloads. Option D \r\nis incorrect because Amazon Redshift is a data warehousing service."
    },
    {
      "question": "Which of the following statements about Amazon DynamoDB table is true?\r\nA. Only one local secondary index is allowed per table.\r\nB. You can create global secondary indexes only when you are creating the table.\r\nC. You can have only one global secondary index.\r\nD. You can create local secondary indexes only when you are creating the table.\r\n",
      "answer": "D. Option D is correct because local secondary indexes on a table are created when the \r\ntable is created. Options A and C are incorrect because you can have five local secondary \r\nindexes or five global secondary indexes per table. Option B is incorrect because you can \r\ncreate global secondary indexes after you have created the table."
    }
  ],
  "Chapter 15": [
    {
      "question": "You are required to set up dynamic scaling using Amazon CloudWatch alarms.\r\nWhich of the following metrics could you monitor to trigger Auto Scaling events to scale \r\nout and scale in your instances?\r\nA. High CPU utilization to trigger scale-in action, and low CPU utilization to trigger \r\nscale-out action\r\nB. High CPU utilization to trigger scale-out action, and low CPU utilization to trigger \r\nscale-in action\r\nC. High latency to trigger a scale-in action, and low latency to trigger a scale-out action\r\nD. None of the above",
      "answer": "B. Option A is incorrect because you do not want to scale in to reduce your capacity when \r\nyou are experiencing a high load. Option C is incorrect because you do not want to scale in \r\nto reduce your capacity when your application is taking a long time to respond. Option D \r\nis incorrect because metrics are required for triggering AWS Auto Scaling events. Option B \r\nis correct because scaling out should occur when more resources are being consumed than \r\nnormal, and scaling in should occur when less resources are being consumed."
    },
    {
      "question": "What is the length of time that metrics are stored for a data point with a period of  \r\n300 seconds (5 minutes) in Amazon CloudWatch?\r\nA. The data point is stored for 3 hours.\r\nB. The data point is stored for 15 days.\r\nC. The data point is stored for 30 days.\r\nD. The data point is stored for 63 days.\r\nE. The data point is stored for 455 days (15 months).",
      "answer": "D. Options A, B, C, and D are all incorrect because data points with a period of 300 sec-\r\nonds are stored for 63 days in Amazon CloudWatch."
    },
    {
      "question": "Which of the following does an AWS CloudTrail event not provide?\r\nA. Who made the request\r\nB. When the request was made\r\nC. What request is being made\r\nD. Why the request was made\r\nE. Which resource was acted on",
      "answer": "D. Option A is incorrect because AWS CloudTrail events show who made the request. \r\nOption B is incorrect because CloudTrail shows when the request was made, and option C \r\nis incorrect because CloudTrail shows what was requested. Option E is incorrect because \r\nCloudTrail shows what resource was acted on. Option D is correct because CloudTrail can \r\nprovide no insight into why a request was made."
    },
    {
      "question": "You must set up centralized logging for an application and create a cost-effective way to \r\narchive logs for compliance purposes.\r\nWhich is the best solution?\r\nA. Install the Amazon CloudWatch agent on your servers to ingest the logs and store them \r\nindefinitely.\r\nB. Configure Amazon CloudWatch to ingest logs from your application servers.\r\nC. Install the Amazon CloudWatch agent on your servers to ingest the logs and set a new \r\nretention period for logs with regular exports to Amazon S3 for archival.\r\nD. None of the above.\r\n",
      "answer": "C. Option A would work; however, it is not the most cost-effective way because logs stored \r\nin CloudWatch cost more than logs stored in Amazon S3. Option B is incorrect because \r\nCloudWatch cannot ingest logs without access to your servers. Option C is correct because \r\narchiving logs from CloudWatch to Amazon S3 reduces overall data storage costs."
    },
    {
      "question": "Which of the following options allow logs and metrics to be ingested into Amazon  \r\nCloudWatch? (Select THREE.)\r\nA. Install the Amazon CloudWatch agent and configure it to ingest logs.\r\nB. Execute API operations to push metrics to Amazon CloudWatch.\r\nC. Configure Amazon CloudWatch to pull logs from servers.\r\nD. Use the AWS CLI to push metrics to Amazon CloudWatch.",
      "answer": "A, B, D. Option C is incorrect because CloudWatch has no way to access data in your \r\napplications or servers. You must push the data either by using the CloudWatch SDK or \r\nAWS CLI or by installing the CloudWatch agent. Option A is correct because the Cloud-\r\nWatch agent is required to send operating system and application logs to CloudWatch. \r\nOption B is likewise correct because metrics logs are sent to CloudWatch using the  \r\nPutMetricData and PutLogEvents API actions. Option D is also correct because the  \r\nAWS CLI can be used to send metrics to CloudWatch using the put-metric-data and  \r\nput-log-events commands."
    },
    {
      "question": "The following are Apache HTTP access logs.\r\nWhich filter pattern would select events matching 404 errors?\r\n127.0.0.1 - - [24/Sep/2013:11:49:52 -0700] \"GET /index.html HTTP/1.1\" 404 287\r\n127.0.0.1 - - [24/Sep/2013:11:49:52 -0700] \"GET /index.html HTTP/1.1\" 404 287\r\n127.0.0.1 - - [24/Sep/2013:11:50:51 -0700] \"GET /~test/ HTTP/1.1\" 200 3\r\n127.0.0.1 - - [24/Sep/2013:11:50:51 -0700] \"GET /favicon.ico HTTP/1.1\" 404 308\r\n127.0.0.1 - - [24/Sep/2013:11:50:51 -0700] \"GET /favicon.ico HTTP/1.1\" 404 308\r\n127.0.0.1 - - [24/Sep/2013:11:51:34 -0700] \"GET /~test/index.html HTTP/1.1\" 200 3\r\nA. 4xx\r\nB. 400\r\nC. 404\r\nD. None of the above",
      "answer": "C. Options A and B are incorrect because the strings must match a filter pattern equal to \r\n404. Option C is correct because 404 matches the error code present in the example logs."
    },
    {
      "question": "You build an application and enable AWS X-Ray tracing. You analyze the service graph and \r\ndetermine that the application requests to Amazon DynamoDB are not performing well and \r\na majority of the issues are purple.\r\nWhat kind of problem is your application experiencing?\r\nA. Throttling\r\nB. Error\r\nC. Faults\r\nD. OK",
      "answer": "A. AWS X-Ray color-codes the response types you get from your services. For 4XX, or \r\nclient-side errors, the circle is orange. Thus, option B is incorrect. Application failures \r\nor faults are red, and successful responses, or 2XX, are green. Thus, options C and D \r\nare incorrect. For throttling, or 5XX series errors, the circle is purple. Thus, option A is \r\ncorrect."
    },
    {
      "question": "Which AWS service enables you to monitor resources and gather statistics, such as CPU \r\nutilization, from a single “pane of glass” interface?\r\nA. AWS CloudTrail logs\r\nB. Amazon CloudWatch alarms\r\nC. Amazon CloudWatch dashboards\r\nD. Amazon CloudWatch Logs",
      "answer": "C. Option A is incorrect because CloudTrail logs list security-related events and do not \r\nprovide a dashboard feature. Option B is incorrect because CloudWatch alarms are used \r\nto notify you when something isn’t operating based on your specifications. Option D is \r\nincorrect because Amazon CloudWatch Logs are for sending and storing server logs to the \r\nCloudWatch service; however, you could use these logs to create a metric and then place \r\nit on the CloudWatch dashboard. Option C is the correct answer. Use CloudWatch dash-\r\nboards to create a single interface where you can monitor all the resources."
    },
    {
      "question": "By default, what is the number of days of AWS account activity that you can view, search, \r\nand download from the AWS CloudTrail event history?\r\nA. 30 days\r\nB. 60 days\r\nC. 75 days\r\nD. 90 days",
      "answer": "D. CloudTrail stores the CloudTrail event history for 90 days; however, if you would like \r\nto store this information permanently, you can create an CloudTrail trail, which stores the \r\nlogs in Amazon S3."
    },
    {
      "question": "Which of the following is not able to access AWS CloudTrail data?\r\nA. AWS CLI\r\nB. AWS Management Console\r\nC. AWS CloudTrail API\r\nD. None of the above",
      "answer": "D. Option C is incorrect because the LookupEvents API action can be used to query event \r\ndata. Options A and B are also incorrect because the AWS CLI and the AWS Management \r\nConsole use the same CloudTrail APIs to query event data. Thus, option D is correct.\r\n914 Appendix ■ Answers to Review Questions"
    },
    {
      "question": "In AWS CloudTrail, which of the following are management events? (Select TWO.)\r\nA. Adding a row to an Amazon DynamoDB table\r\nB. Modifying an Amazon S3 bucket policy\r\nC. Uploading an object to an Amazon S3 bucket\r\nD. Creating an Amazon Relational Database Service (Amazon RDS) database instance\r\nE. Sending a notification to Amazon Simple Notification Service (Amazon SNS)",
      "answer": "B, D. Management events are operations performed on resources in your AWS account. \r\nData events are operations performed on data stored in AWS resources. For example, \r\nmodifying an object in Amazon S3 would qualify as a data event, and changing a bucket \r\npolicy would qualify as a management event. Because options A, C, and E involve sending \r\nor receiving data, not modifying or creating AWS resources, they are data events. Thus, \r\noptions B and D are correct."
    },
    {
      "question": "Suppose that you have a custom web application running on an Amazon Elastic Compute \r\nCloud (Amazon EC2) instance.\r\nWhat steps are needed to configure this instance to send custom application logs to Amazon \r\nCloudWatch Logs? (Select THREE.)\r\nA. Install the Amazon CloudWatch Logs agent.\r\nB. Attach an Elastic IP address to your Amazon EC2 instance.\r\nC. Configure the agent to send specific logs.\r\nD. Start the agent.\r\nE. Install the AWS Systems Manager agent.",
      "answer": "A, C, D. When installing the CloudWatch Logs agent, no additional networking con-\r\nfiguration is required as long as your instance can reach the CloudWatch API endpoint. \r\nTherefore, option B is incorrect. You can use AWS Systems Manager to install and start the \r\nagent, but it is not required to install the Systems Manager agent alongside the CloudWatch \r\nLogs agent; thus, option E is incorrect. When installing the agent, you must configure the \r\nspecific logs to send. The agent must be started before new log data is sent to CloudWatch \r\nLogs."
    },
    {
      "question": "Which of the following are not supported Amazon CloudWatch alarm actions?\r\nA. AWS Lambda functions\r\nB. Amazon Simple Notification Service (Amazon SNS) topics\r\nC. Amazon Elastic Compute Cloud (Amazon EC2) actions\r\nD. EC2 Auto Scaling actions",
      "answer": "A. CloudWatch alarms support triggering actions in Amazon EC2, EC2 Auto Scaling, \r\nand Amazon SNS. Thus, options B, C, and D are incorrect. It is possible to trigger AWS \r\nLambda functions from an alarm, but only by first sending the alarm notification to an \r\nAmazon SNS topic. Thus, option A is correct."
    },
    {
      "question": "Which of the following Amazon Elastic Compute Cloud (Amazon EC2) metrics is not \r\ndirectly available through Amazon CloudWatch metrics?\r\nA. CPU utilization\r\nB. Network traffic in/out\r\nC. Disk I/O\r\nD. Memory (RAM) utilization",
      "answer": "D. CPU, network, and disk activity are metrics that are visible to the underlying host for an \r\ninstance. Thus, options A, B, and C are incorrect. Because memory is allocated in a single \r\nblock to an instance and is managed by the guest OS, the underlying host does not have \r\nvisibility into consumption. This metric would have to be delivered to CloudWatch as a \r\ncustom metric by using the agent. Thus, option D is correct."
    },
    {
      "question": "Which of the following is the correct Amazon CloudWatch metric namespace for Amazon \r\nElastic Compute Cloud (Amazon EC2) instances?\r\nA. AWS/EC2\r\nB. Amazon/EC2\r\nC. AWS/EC2Instance\r\nD. Amazon/EC2Instance\r\n",
      "answer": "A. No namespace starts with an Amazon prefix; therefore, options B and D are incorrect. \r\nOption C is incorrect because namespaces are specific to a service (Amazon EC2), not a \r\nresource (an instance). Option A is correct because the Amazon EC2 service uses the AWS \r\nprefix, followed by EC2."
    }
  ],
  "Chapter 16": [
    {
      "question": "You are developing an application that will run across dozens of instances. It uses \r\nsome components from a legacy application that requires some configuration files to \r\nbe copied from a central location and held on a volume local to each of the instances. \r\nYou plan to modify your application with a new component in the future that will hold \r\nthis configuration in Amazon DynamoDB. Which storage option should you use in the \r\ninterim to provide the lowest cost and the lowest latency for your application to access the \r\nconfiguration files?\r\nA. Amazon S3\r\nB. Amazon EBS\r\nC. Amazon EFS\r\nD. Amazon EC2 instance store",
      "answer": "D. Amazon EC2 instance store is directly attached to the instance, which gives you the \r\nlowest latency between the disk and your application. Instance store is also provided at no \r\nadditional cost on instance types that have it available, so this is the lowest-cost option. \r\nAdditionally, because the data is being retrieved from somewhere else, it can be copied \r\nback to an instance as needed. Option A is incorrect because Amazon S3 cannot be directly \r\nmounted to an Amazon EC2 instance. Options B and C are incorrect because Amazon EBS \r\nand Amazon EFS would be higher-cost options, with a higher latency than an instance \r\nstore."
    },
    {
      "question": "Similar to SQL, Amazon DynamoDB provides several operations for reading the data. \r\nWhich operation is the most efficient way to retrieve a single item?\r\nA. Query\r\nB. Scan\r\nC. GetItem\r\nD. Join",
      "answer": "C. GetItem retrieves a single item from a table. This is the most efficient way to read a \r\nsingle item because it provides direct access to the physical location of the item. Options A \r\nand B are incorrect. Query retrieves all the items that have a specific partition key. Within \r\nthose items, you can apply a condition to the sort key and retrieve only a subset of the data. Query provides quick, efficient access to the partitions where the data is stored. Scan \r\nretrieves all of the items in the specified table, and it can consume large amounts of system \r\nresources based on the size of the table. Option D is incorrect. DynamoDB is a nonrela-\r\ntional NoSQL database, and it does not support table joins. Instead, applications read data \r\nfrom one table at a time."
    },
    {
      "question": "AWS Trusted Advisor offers a rich set of best practice checks and recommendations across \r\nfive categories: cost optimization, security, fault tolerance, performance, and service limits. \r\nWhich of the following checks is NOT under Cost and Performance categories?\r\nA. Amazon EBS Provisioned IOPS (SSD) volume attachment configuration\r\nB. Amazon CloudFront header forwarding and cache hit ratio\r\nC. Amazon EC2 Availability Zone balance\r\nD. Unassociated Elastic IP address",
      "answer": "C. Option C is a fault-tolerance check. By launching instances in multiple Availability \r\nZones in the same region, you help protect your applications from a single point of failure. \r\nOptions A and B are performance checks. Provisioned IOPS volumes in the Amazon \r\nEBS are designed to deliver the expected performance only when they are attached to an \r\nAmazon EBS optimized instance. Some headers, such as Date or User-Agent, significantly \r\nreduce the cache hit ratio (the proportion of requests that are served from a CloudFront \r\nedge cache). This increases the load on your origin and reduces performance because \r\nCloudFront must forward more requests to your origin. Option D is a cost check. Elastic IP \r\naddresses are static IP addresses designed for dynamic cloud computing. A nominal charge \r\nis imposed for an Elastic IP address that is not associated with a running instance."
    },
    {
      "question": "Which of the following common partition schemas includes a partition key design that \r\ndistributes I/O requests evenly across partitions and uses provisioned I/O capacity of an \r\nAmazon DynamoDB table efficiently?\r\nA. Status code, where there are only a few possible status codes\r\nB. User ID, where the application has many users\r\nC. Item creation date, rounded to the nearest time period\r\nD. Device ID, where even if there are many devices tracked, one is by far more popular \r\nthan all the others\r\n",
      "answer": "B. Options A, C, and D are incorrect because partition keys used in these options could \r\ncause “hot” (heavily requested) partition keys because of lack of uniformity. Design your \r\napplication for uniform activity across all logical partition keys in the table and its second-\r\nary indexes. Use distinct values for each item."
    },
    {
      "question": "You are developing an application that consists of a set of Amazon EC2 instances hosting \r\na web layer and a database hosting a MySQL instance. You are required to add a layer that \r\ncan be used to ensure that the most frequently accessed data from the database is fetched \r\nin a faster and more efficient manner. Which of the following can be used to store the \r\nfrequently accessed data?\r\nA. Amazon Simple Queue Service (Amazon SQS) queue\r\nB. Amazon Simple Notification Service (Amazon SNS) topic\r\nC. Amazon CloudFront distribution\r\nD. Amazon ElastiCache instance",
      "answer": "D. Option A is incorrect because SQS is a messaging service. Option B is incorrect because \r\nSNS is a notification service. Option C is incorrect because CloudFront is a web distribu-\r\ntion service. Option D is correct because ElastiCache improves the performance of your \r\napplication by retrieving data from high throughput and low latency in-memory data \r\nstores. For details, see https://aws.amazon.com/elasticache."
    },
    {
      "question": "You have an application deployed to the AWS platform. The application makes requests \r\nto an Amazon Simple Storage Service (Amazon S3) bucket. After monitoring the Amazon \r\nCloudWatch metrics, you notice that the number of GET requests has suddenly spiked. \r\nWhich of the following can be used to optimize Amazon S3 cost and performance?\r\nA. Add Amazon ElastiCache in front of the S3 bucket.\r\nB. Use Amazon DynamoDB instead of Amazon S3.\r\nC. Place an Amazon CloudFront distribution in front of the S3 bucket.\r\nD. Place an Elastic Load Balancing load balancer in front of the S3 bucket.",
      "answer": "C. Option C is correct because CloudFront optimizes performance if your workload is \r\nmainly sending GET requests. There are also fewer direct requests to Amazon S3, which \r\nreduces cost. For details, see https://docs.aws.amazon.com/AmazonS3/latest/dev/\r\nrequest-rate-perf-considerations.html."
    },
    {
      "question": "You are writing an application that will store data in an Amazon DynamoDB table. The \r\nratio of read operations to write operations will be 1,000 to 1, with the same data being \r\naccessed frequently. Which feature or service should you enable on the DynamoDB table to \r\noptimize performance and minimize costs?\r\nA. Amazon DynamoDB Auto Scaling\r\nB. Amazon DynamoDB cross-region replication\r\nC. Amazon DynamoDB Streams\r\nD. Amazon DynamoDB Accelerator",
      "answer": "D. Option A is incorrect because AWS Auto Scaling is optimal for unpredictable work-\r\nloads. Option B is incorrect because cross-region replication is better for disaster recovery \r\nscenarios. Option C is incorrect because DynamoDB streams are better suited to stream \r\ndata to other sources. Option D is correct because Amazon DynamoDB Accelerator (DAX) \r\nprovides fast in-memory performance. For details, see https://docs.aws.amazon.com/\r\namazondynamodb/latest/developerguide/DAX.html."
    },
    {
      "question": "A developer is migrating an on-premises web application to the AWS Cloud. The \r\napplication currently runs on a 32-processor server and stores session state in memory. On \r\nMondays, the server runs at 80 percent CPU utilization, but at only about 5 percent CPU \r\nutilization at other times. How should the developer change the code to optimize running \r\nin the AWS Cloud?\r\nA. Store session state on the Amazon EC2 instance store.\r\nB. Encrypt the session state in memory.\r\nC. Store session state in an Amazon ElastiCache cluster.\r\nD. Compress the session state in memory.",
      "answer": "C. Option A is incorrect because EC2 instance store is too volatile to be optimal. Option \r\nB is incorrect because this is a security solution and will not impact performance positively. \r\nOption C is correct because ElastiCache is ideal for handling session state. You can abstract \r\nthe HTTP sessions from the web servers by using Redis and Memcached. Option D is \r\nincorrect because compression is not the optimal solution given the choices. For details, see \r\nhttps://aws.amazon.com/caching/session-management/."
    },
    {
      "question": "A company is using an ElastiCache cluster in front of their Amazon RDS instance. The \r\ncompany would like you to implement logic into the code so that the cluster retrieves data \r\nfrom Amazon RDS only when there is a cache miss. Which strategy can you implement to \r\nachieve this?\r\nA. Error retries\r\nB. Lazy loading\r\nC. Exponential backoff\r\nD. Write-through",
      "answer": "B. Option B is correct because lazy loading only loads data into the cache when necessary. \r\nThis avoids filling up the cache with data that isn’t requested. Options A, C, and D are \r\n916 Appendix ■ Answers to Review Questions\r\nincorrect because they do not match the requirement of the question. For details, see  \r\nhttps://docs.aws.amazon.com/AmazonElastiCache/latest/mem-ug/ \r\nStrategies.html."
    },
    {
      "question": "Your application will be hosted on an Amazon EC2 instance, which will be part of an AWS \r\nAuto Scaling group. The application must fetch the private IP of the instance. Which of the \r\nfollowing can achieve this?\r\nA. Query the instance metadata.\r\nB. Query the instance user data.\r\nC. Have the application run ifconfig.\r\nD. Have an administrator get the IP address from the Amazon EC2 console.",
      "answer": "A. Option A is correct because information about the instance, such as private IP, is stored \r\nin the instance metadata. Option B is incorrect because private IP information is not  \r\nstored in the instance user data. Option C is incorrect because running ifconfig is manual \r\nand not automated. Option D is incorrect because it is not clear on what type of instance \r\nthe application is running. For details, see https://docs.aws.amazon.com/AWSEC2/ \r\nlatest/UserGuide/ec2-instance-metadata.html."
    },
    {
      "question": "You just developed code in AWS Lambda that uses recursive functions. You see some \r\nthrottling errors in the metrics. Which of the following should you do to resolve the issue?\r\nA. Use API Gateway to call the recursive code.\r\nB. Use versioning for the recursive function.\r\nC. Place the recursive function in a separate package.\r\nD. Avoid using recursive code in your function.",
      "answer": "D. Options A, B, and C are incorrect because they are not recommended best practices. \r\nOption D is correct because it is one of the recommendations in the best practices docu-\r\nmentation, “Avoid using recursive code.” For details, see https://docs.aws.amazon.com/\r\nlambda/latest/dg/best-practices.html."
    },
    {
      "question": "A production application is making calls to an Amazon Relational Database Service \r\n(Amazon RDS) instance. The application’s reporting module is experiencing heavy traffic, \r\ncausing performance issues. How can the application be optimized to alleviate this issue?\r\nA. Move the database to Amazon DynamoDB, and point the reporting module to the new \r\nDynamoDB table.\r\nB. Enable Multi-AZ for the database, and point the reporting module to the secondary \r\ndatabase.\r\nC. Enable read replicas for the database, and point the reporting module to the read \r\nreplica.\r\nD. Place an Elastic Load Balancing load balancer in front of the reporting part of the \r\napplication.",
      "answer": "C. Option A is incorrect because changing the entire architecture is not ideal. Option B is \r\nincorrect because Multi-AZ is used for fault tolerance. Option C is correct because loads \r\ncan be reduced by routing read queries from your application to the read replica. Option D \r\nis incorrect because using an Elastic Load Balancing load balancer will not reduce the query \r\nload. For details, see https://aws.amazon.com/rds/details/read-replicas/."
    },
    {
      "question": "Your application uses Amazon S3 buckets. You have users in other countries accessing \r\nobjects in those buckets. What can you do to reduce latency for those users outside of your \r\ncountry?\r\nA. Host a static website.\r\nB. Change the storage class.\r\nC. Enable cross-region replication.\r\nD. Enable encryption.\r\n",
      "answer": "C. Option A is incorrect because this is relevant only when you need a static website.  \r\nOption B is incorrect because changing the storage class does not help with latency. Option \r\nC is correct because cross-region replication maintains object copies in regions that are  \r\ngeographically closer to your users, reducing latency. Option D is incorrect because  \r\nencryption is necessary only for securing data at rest. For details, see https://docs.aws \r\n.amazon.com/AmazonS3/latest/dev/crr.html."
    },
    {
      "question": "You have an application that uploads objects to Amazon S3 between 200–500 MB. The \r\nprocess takes longer than expected, and you want to improve the performance of the \r\napplication. Which of the following would you consider?\r\nA. Enable versioning on the bucket.\r\nB. Use the multipart upload API.\r\nC. Write the items in batches for better performance.\r\nD. Create multiple threads to upload the objects.",
      "answer": "B. Options A, C, and D are incorrect because they are not optimal for handling large \r\nobject uploads to Amazon S3. Option B is correct because a multipart upload enables \r\nyou to upload large objects in parts to Amazon S3. For details, see https://docs \r\n.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html."
    },
    {
      "question": "You must bootstrap your application script to instances that are launched inside an AWS \r\nAuto Scaling group. Which is the most optimal way to achieve this?\r\nA. Create a Lambda function to install the script.\r\nB. Place a scheduled task on the instance that starts on boot.\r\nC. Place the script in the instance user data.\r\nD. Place the script in the instance metadata.\r\n",
      "answer": "C. Option A is incorrect because this is not the optimal approach for bootstrapping. Option B \r\nis incorrect because, while possible, bootstrapping in the user data is optimal. Option C is \r\ncorrect because instance user data is used to perform common automated  \r\nconfiguration tasks and run scripts after boot. Option D is incorrect because  \r\nbootstrapping is done in instance user data, not instance metadata. For details, see \r\nhttps://docs.aws.amazon.com/AWSEC2/latest/UserGuide/user-data.html.\r\n"
    }
  ]
}